@online{asai2024,
  title = {{{OpenScholar}}: {{Synthesizing Scientific Literature}} with {{Retrieval-augmented LMs}}},
  shorttitle = {{{OpenScholar}}},
  author = {Asai, Akari and He, Jacqueline and Shao, Rulin and Shi, Weijia and Singh, Amanpreet and Chang, Joseph Chee and Lo, Kyle and Soldaini, Luca and Feldman, Sergey and D'arcy, Mike and Wadden, David and Latzke, Matt and Tian, Minyang and Ji, Pan and Liu, Shengyan and Tong, Hao and Wu, Bohao and Xiong, Yanyu and Zettlemoyer, Luke and Neubig, Graham and Weld, Dan and Downey, Doug and Yih, Wen-tau and Koh, Pang Wei and Hajishirzi, Hannaneh},
  date = {2024-11-21},
  eprint = {2411.14199},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2411.14199},
  url = {http://arxiv.org/abs/2411.14199},
  urldate = {2024-11-24},
  abstract = {Scientific progress depends on researchers' ability to synthesize the growing body of literature. Can large language models (LMs) assist scientists in this task? We introduce OpenScholar, a specialized retrieval-augmented LM that answers scientific queries by identifying relevant passages from 45 million open-access papers and synthesizing citation-backed responses. To evaluate OpenScholar, we develop ScholarQABench, the first large-scale multi-domain benchmark for literature search, comprising 2,967 expert-written queries and 208 long-form answers across computer science, physics, neuroscience, and biomedicine. On ScholarQABench, OpenScholar-8B outperforms GPT-4o by 5\% and PaperQA2 by 7\% in correctness, despite being a smaller, open model. While GPT4o hallucinates citations 78 to 90\% of the time, OpenScholar achieves citation accuracy on par with human experts. OpenScholar's datastore, retriever, and self-feedback inference loop also improves off-the-shelf LMs: for instance, OpenScholar-GPT4o improves GPT-4o's correctness by 12\%. In human evaluations, experts preferred OpenScholar-8B and OpenScholar-GPT4o responses over expert-written ones 51\% and 70\% of the time, respectively, compared to GPT4o's 32\%. We open-source all of our code, models, datastore, data and a public demo.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Digital Libraries,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\A28QUTKK\\Asai 等 - 2024 - OpenScholar Synthesizing Scientific Literature wi.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\SKCX273M\\2411.html}
}

@article{berstein2011,
  title = {Chapitre 7. La crise des politiques internationale et militaire et l’effondrement de la IIIe République},
  author = {Berstein, Serge},
  date = {2011},
  journaltitle = {Cursus},
  pages = {185--204},
  publisher = {Armand Colin},
  url = {https://shs.cairn.info/la-france-des-annees-30--9782200248895-page-185},
  urldate = {2025-01-01},
  langid = {french},
  file = {C:\Users\bdh19\Zotero\storage\L6QI55SL\la-france-des-annees-30--9782200248895-page-185.html}
}

@article{blei2003,
  title = {Latent dirichlet allocation},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  date = {2003-03-01},
  journaltitle = {The Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume = {3},
  pages = {993--1022},
  issn = {1532-4435},
  abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  issue = {null},
  file = {C:\Users\bdh19\Zotero\storage\L55QJ9PZ\Blei 等 - 2003 - Latent dirichlet allocation.pdf}
}

@inproceedings{blei2006,
  title = {Dynamic topic models},
  booktitle = {Proceedings of the 23rd international conference on {{Machine}} learning  - {{ICML}} '06},
  author = {Blei, David M. and Lafferty, John D.},
  date = {2006},
  pages = {113--120},
  publisher = {ACM Press},
  location = {Pittsburgh, Pennsylvania},
  doi = {10.1145/1143844.1143859},
  url = {http://portal.acm.org/citation.cfm?doid=1143844.1143859},
  urldate = {2024-05-14},
  abstract = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR’ed archives of the journal Science from 1880 through 2000.},
  eventtitle = {the 23rd international conference},
  isbn = {978-1-59593-383-6},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\bdh19\Zotero\storage\75U8C7RK\Blei 和 Lafferty - 2006 - Dynamic topic models.pdf}
}

@online{gao2024,
  title = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}},
  author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
  date = {2024-03-27},
  eprint = {2312.10997},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.10997},
  url = {http://arxiv.org/abs/2312.10997},
  urldate = {2025-01-25},
  abstract = {Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\L8WSL5I3\\Gao 等 - 2024 - Retrieval-Augmented Generation for Large Language .pdf;C\:\\Users\\bdh19\\Zotero\\storage\\2TWWFRPT\\2312.html}
}

@online{grootendorst2022a,
  title = {{{BERTopic}}: {{Neural}} topic modeling with a class-based {{TF-IDF}} procedure},
  shorttitle = {{{BERTopic}}},
  author = {Grootendorst, Maarten},
  date = {2022-03-11},
  eprint = {2203.05794},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.05794},
  url = {http://arxiv.org/abs/2203.05794},
  urldate = {2025-01-14},
  abstract = {Topic models can be useful tools to discover latent topics in collections of documents. Recent studies have shown the feasibility of approach topic modeling as a clustering task. We present BERTopic, a topic model that extends this process by extracting coherent topic representation through the development of a class-based variation of TF-IDF. More specifically, BERTopic generates document embedding with pre-trained transformer-based language models, clusters these embeddings, and finally, generates topic representations with the class-based TF-IDF procedure. BERTopic generates coherent topics and remains competitive across a variety of benchmarks involving classical models and those that follow the more recent clustering approach of topic modeling.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\Q9A5BQZT\\Grootendorst - 2022 - BERTopic Neural topic modeling with a class-based.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\8PKAVVH8\\2203.html}
}

@article{guieu2016,
  title = {Gagner la paix. 1914-1929},
  author = {Guieu, Jean-Michel},
  date = {2016},
  publisher = {Le Seuil},
  doi = {10.3917/ls.guieu.2013.01},
  url = {https://shs.cairn.info/gagner-la-paix-1914-1929--9782021001457},
  urldate = {2025-07-19},
  abstract = {L'Univers historique},
  langid = {french},
  file = {C:\Users\bdh19\Zotero\storage\CA2HQ9MZ\gagner-la-paix-1914-1929--9782021001457.html}
}

@online{guo2024,
  title = {{{LightRAG}}: {{Simple}} and {{Fast Retrieval-Augmented Generation}}},
  shorttitle = {{{LightRAG}}},
  author = {Guo, Zirui and Xia, Lianghao and Yu, Yanhua and Ao, Tu and Huang, Chao},
  date = {2024-11-07},
  eprint = {2410.05779},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.05779},
  url = {http://arxiv.org/abs/2410.05779},
  urldate = {2025-02-11},
  abstract = {Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs. However, existing RAG systems have significant limitations, including reliance on flat data representations and inadequate contextual awareness, which can lead to fragmented answers that fail to capture complex inter-dependencies. To address these challenges, we propose LightRAG, which incorporates graph structures into text indexing and retrieval processes. This innovative framework employs a dual-level retrieval system that enhances comprehensive information retrieval from both low-level and high-level knowledge discovery. Additionally, the integration of graph structures with vector representations facilitates efficient retrieval of related entities and their relationships, significantly improving response times while maintaining contextual relevance. This capability is further enhanced by an incremental update algorithm that ensures the timely integration of new data, allowing the system to remain effective and responsive in rapidly changing data environments. Extensive experimental validation demonstrates considerable improvements in retrieval accuracy and efficiency compared to existing approaches. We have made our LightRAG open-source and available at the link: https://github.com/HKUDS/LightRAG.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\3FRGHJJH\\Guo 等 - 2024 - LightRAG Simple and Fast Retrieval-Augmented Gene.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\UAZQ3A7E\\2410.html}
}

@online{lewis2021,
  title = {Retrieval-{{Augmented Generation}} for {{Knowledge-Intensive NLP Tasks}}},
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
  date = {2021-04-12},
  eprint = {2005.11401},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2005.11401},
  urldate = {2024-11-18},
  abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\4SM28DS9\\Lewis 等 - 2021 - Retrieval-Augmented Generation for Knowledge-Inten.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\HBZVGBBW\\2005.html}
}

@online{martin2020,
  title = {{{CamemBERT}}: a {{Tasty French Language Model}}},
  shorttitle = {{{CamemBERT}}},
  author = {Martin, Louis and Muller, Benjamin and Suárez, Pedro Javier Ortiz and Dupont, Yoann and Romary, Laurent and family=Clergerie, given=Éric Villemonte, prefix=de la, useprefix=false and Seddah, Djamé and Sagot, Benoît},
  date = {2020-05-21},
  eprint = {1911.03894},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/1911.03894},
  urldate = {2024-11-18},
  abstract = {Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models --in all languages except English-- very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\8NBBMJ2W\\CAIH_paper.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\EENBGDWZ\\Martin 等 - 2020 - CamemBERT a Tasty French Language Model.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\WLYP8FDP\\1911.html}
}

@article{milza2019,
  title = {Les relations internationales de 1918 à 1939},
  author = {Milza, Pierre},
  date = {2019},
  publisher = {Armand Colin},
  url = {https://shs.cairn.info/les-relations-internationales-de-1918-a-1939--9782200625863},
  urldate = {2025-07-19},
  abstract = {Cursus},
  langid = {french},
  file = {C:\Users\bdh19\Zotero\storage\G74TYVLB\les-relations-internationales-de-1918-a-1939--9782200625863.html}
}

@article{montero2001,
  title = {La France de 1914 à 1945},
  author = {Montero, Muriel},
  date = {2001},
  publisher = {Armand Colin},
  url = {https://shs.cairn.info/la-france-de-1914-a-1945--9782200252694},
  urldate = {2025-07-19},
  abstract = {Hors collection},
  langid = {french},
  file = {C:\Users\bdh19\Zotero\storage\YDIAF3II\la-france-de-1914-a-1945--9782200252694.html}
}

@book{morel2024,
  title = {Le Parlement, temple de la République: De 1789 à nos jours},
  shorttitle = {Le Parlement, temple de la République},
  author = {Morel, Benjamin},
  date = {2024-07-01T00:00:00+02:00},
  eprint = {zmYUEQAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {Humensis},
  abstract = {Institution marginalisée, parfois méprisée de la Ve République, le Parlement montre pourtant que rien ne peut se faire sans lui. La chose est d’autant plus vraie depuis sa récente dissolution décidée par Emmanuel Macron. Son histoire se confond avec celle de la République. Toutefois, son rôle et la manière dont on conçoit sa fonction ont évolué. Représentation imparfaite d’un peuple qui devait le contrôler, il est devenu un rouage administratif sous l’Empire. Chambre d’aristocrates puis de notables, c’est par la délibération qu’il acquiert, au XIXe siècle, un rôle central. Chargés de définir l’intérêt général, députés et sénateurs développent les techniques de débat et d’éloquence, participant à créer une culture parlementaire. Derrière cette histoire chaotique réside celle de la République et de la démocratie. Même quand, comme sous la Restauration, les chambres se voulurent parangon de la réaction, leurs actions poussèrent à une libéralisation du régime. Même quand le Second Empire voulut les réduire, il comprit qu’il ne pourrait survivre qu’en s’appuyant sur les assemblées. Le fait parlementaire est têtu. Attaquées ou données pour mortes, les chambres ont survécu aux rois et aux empereurs, aux guerres et aux crises. Fil rouge et force dynamique de l’histoire de France depuis deux siècles, le Parlement est au centre de notre modernité politique. En voici son histoire, publique, secrète, intime, vivante, totale.},
  isbn = {978-2-37933-984-4},
  langid = {french},
  pagetotal = {234},
  keywords = {History / General}
}

@article{mougel2018,
  title = {Chapitre VII. L’entre-deux-guerres (1918-1939)},
  author = {Mougel, François-Charles and Pacteau, Séverine},
  date = {2018-10-01},
  journaltitle = {Que sais-je ?},
  volume = {14},
  pages = {64--74},
  publisher = {Presses Universitaires de France},
  issn = {0768-0066},
  url = {https://shs.cairn.info/histoire-des-relations-internationales-de-1815--9782130809807-page-64},
  urldate = {2025-07-19},
  isbn = {9782130809807},
  langid = {french},
  file = {C:\Users\bdh19\Zotero\storage\IHLI7463\histoire-des-relations-internationales-de-1815--9782130809807-page-64.html}
}

@article{pellet,
  title = {Intelligence artificielle générative et recherche historique : enjeux, potentiels et limites. Application de la RAG aux débats parlementaires français de la Troisième République (1881-1940)},
  author = {Pellet, Aurélien and Perez, Julien and Puren, Marie},
  abstract = {Ce travail s’intéresse à l’analyse des débats parlementaires de la Troisième République (1870-1940), en utilisant les avancées récentes des modèles génératifs de texte basée sur la recherche d’information. Notre objectif est d’évaluer les capacités et les limites de cette approche pour modéliser et interroger ce vaste corpus textuel, de façon à mieux d’appréhender des questions complexes telles que les dynamiques politiques, les thématiques récurrentes, ainsi que les facteurs influençant les prises de décision législatives. En combinant des techniques d’extraction automatique d’information et des modèles génératifs, notre but est de structurer les débats, tout en assurant la traçabilité des sources. Dans ce papier, nous détaillons par ailleurs un sous-problème intimement lié à ces questions, à savoir les stratégies de segmentation des documents longs, avec une méthode d’évaluation par génération automatique de questions réponses.},
  langid = {french},
  file = {C:\Users\bdh19\Zotero\storage\K9B7MPEC\Pellet 等 - Intelligence artificielle générative et recherche .pdf}
}

@online{li2024a,
  title = {{{GraphRAG}}: {{Improving}} global search via dynamic community selection},
  shorttitle = {{{GraphRAG}}},
  author = {Li, Bryan and Trinh, Ha and Edge, Darren and Larson, Jonathan},
  date = {2024-11-15T16:52:47+00:00},
  url = {https://www.microsoft.com/en-us/research/blog/graphrag-improving-global-search-via-dynamic-community-selection/},
  urldate = {2025-02-06},
  abstract = {Retrieval-augmented generation (RAG) helps AI systems provide more information to a large language model (LLM) when generating responses to user queries. A new method for conducting “global” queries can optimize the performance of global search in GraphRAG.},
  langid = {english},
  organization = {Microsoft Research},
  file = {C:\Users\bdh19\Zotero\storage\CCNFNL8U\graphrag-improving-global-search-via-dynamic-community-selection.html}
}

@online{larson2024,
  title = {{{GraphRAG}}: {{A}} new approach for discovery using complex information},
  shorttitle = {{{GraphRAG}}},
  author = {Larson, Jonathan and Truitt, Steven},
  date = {2024-02-13T20:00:00+00:00},
  url = {https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/},
  urldate = {2025-02-06},
  abstract = {Microsoft is transforming retrieval-augmented generation with GraphRAG, using LLM-generated knowledge graphs to significantly improve Q\&A when analyzing complex information and consistently outperforming baseline RAG. Get the details.},
  langid = {english},
  organization = {Microsoft Research},
  file = {C:\Users\bdh19\Zotero\storage\QBNBQUKQ\graphrag-unlocking-llm-discovery-on-narrative-private-data.html}
}

@online{whiting2024,
  title = {{{DRIFT}} builds on {{GraphRAG}} methods, combining global and local search},
  author = {Whiting, Julian and Fernández, Alonso Guevara and Trinh, Ha and Larson, Jonathan},
  date = {2024-10-31T19:23:03+00:00},
  url = {https://www.microsoft.com/en-us/research/blog/introducing-drift-search-combining-global-and-local-search-methods-to-improve-quality-and-efficiency/},
  urldate = {2025-02-06},
  abstract = {GraphRAG leverages semantic structuring of data to generate responses to complex user queries. A collaboration with Uncharted expands the frontiers of this technology, developing a new approach to processing local queries: DRIFT search:},
  langid = {english},
  organization = {Microsoft Research},
  file = {C:\Users\bdh19\Zotero\storage\8CJ2XSQZ\introducing-drift-search-combining-global-and-local-search-methods-to-improve-quality-and-effic.html}
}


@inproceedings{puren2023a,
  title = {Explorer les débats parlementaires français de la {{Troisième République}} par leurs sujets},
  booktitle = {Humanistica 2023},
  author = {Puren, Marie and Pellet, Aurélien},
  date = {2023-06},
  series = {Corpus},
  publisher = {Association francophone des humanités numériques},
  location = {Genève, Switzerland},
  url = {https://hal.science/hal-04128262},
  urldate = {2024-11-18},
  abstract = {Cet article compare trois méthodes pour explorer de grands corpus de documents historiques par leurs sujets. Nous travaillons ici sur les débats parlementaires français de la Troisième République, qui se prêtent particulièrement bien à ce type d'analyse. Après avoir présenté le contexte de cette étude, nous exposons les résultats obtenus avec trois méthodes issues du traitement automatique des langues et appliquées sur des textes publiés entre 1876 et 1914 : l'allocation de Dirichlet latente, les plongements de mots et le Transfer Learning.},
  keywords = {Débats parlementaires,Digital humanities,French Third Republic,Humanités numériques,Parliamentary debates,Troisième république},
  file = {C:\Users\bdh19\Zotero\storage\H9US75AK\Puren 和 Pellet - 2023 - Explorer les débats parlementaires français de la .pdf}
}

@online{ru2024,
  title = {{{RAGChecker}}: {{A Fine-grained Framework}} for {{Diagnosing Retrieval-Augmented Generation}}},
  shorttitle = {{{RAGChecker}}},
  author = {Ru, Dongyu and Qiu, Lin and Hu, Xiangkun and Zhang, Tianhang and Shi, Peng and Chang, Shuaichen and Jiayang, Cheng and Wang, Cunxiang and Sun, Shichao and Li, Huanyu and Zhang, Zizhao and Wang, Binjie and Jiang, Jiarong and He, Tong and Wang, Zhiguo and Liu, Pengfei and Zhang, Yue and Zhang, Zheng},
  date = {2024-08-17},
  eprint = {2408.08067},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.08067},
  url = {http://arxiv.org/abs/2408.08067},
  urldate = {2025-02-07},
  abstract = {Despite Retrieval-Augmented Generation (RAG) showing promising capability in leveraging external knowledge, a comprehensive evaluation of RAG systems is still challenging due to the modular nature of RAG, evaluation of long-form responses and reliability of measurements. In this paper, we propose a fine-grained evaluation framework, RAGChecker, that incorporates a suite of diagnostic metrics for both the retrieval and generation modules. Meta evaluation verifies that RAGChecker has significantly better correlations with human judgments than other evaluation metrics. Using RAGChecker, we evaluate 8 RAG systems and conduct an in-depth analysis of their performance, revealing insightful patterns and trade-offs in the design choices of RAG architectures. The metrics of RAGChecker can guide researchers and practitioners in developing more effective RAG systems. This work has been open sourced at https://github.com/amazon-science/RAGChecker.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\WI3VVVKZ\\Ru 等 - 2024 - RAGChecker A Fine-grained Framework for Diagnosin.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\ZTFUMKKM\\2408.html}
}

@unpublished{silvestredesacy2024,
  title = {Pre-targeted-{{RAG}} - {{Retrieval Augmented Generation}} sur des groupes pré-ciblés de communautés d'articles de recherche},
  author = {Silvestre de Sacy, Antoine and Faci, Adam and Pouyllau, Stéphane and Maronet, Léa},
  date = {2024-10-18},
  doi = {10.5281/zenodo.13950650},
  url = {https://zenodo.org/records/13950650},
  urldate = {2025-02-19},
  abstract = {Présentation de l'équipe du HNLab (IR* Huma-Num, UAR 3598, CNRS) sur une méthodologie de RAG pré-ciblé lors du colloque international en linguistique ColDoc~“La linguistique dans une ère nouvelle : discours, méthodes et technologies dans le paysage contemporain”~ (https://coldoc2024.sciencesconf.org/).},
  file = {C:\Users\bdh19\Zotero\storage\NAKCLTGG\Silvestre de Sacy 等 - 2024 - Pre-targeted-RAG - Retrieval Augmented Generation .pdf}
}

@inproceedings{tran2024,
  title = {Retrieval {{Augmented Generation}} for {{Historical Newspapers}}},
  author = {Tran, The Trung and González-Gallardo, Carlos-Emiliano and Doucet, Antoine},
  date = {2024-12-16},
  url = {https://univ-tours.hal.science/hal-04796088},
  urldate = {2025-06-10},
  abstract = {Nowadays, the accessibility and long-term preservation of historical records are significantly impacted by the sharp increase in the digitization of these archives. This shift creates new opportunities for researchers and students in multiple disciplines to broaden their knowledge or conduct multidisciplinary research. However, given the vast amount of data that needs to be analyzed, using this knowledge is not easy. Different natural language processing tasks such as named entity recognition, entity linking, and article separation have been developed to make this accessibility easier for the public by extracting information and structuring data. However, historical newspaper article aggregation is still unexplored. In this work, we demonstrate the potential of the retrieval-augmented generation framework that integrates large language models (LLMs), a semantic retrieval module, and knowledge bases to create a system capable of aggregating historical newspaper articles. In addition, we propose a set of metrics that permit evaluating these generative systems without requiring any ground truth. The results of our proposed RAG pipeline are promising at this early stage of the system. They show that semantic retrieval with the help of reranking and additional information (NER) reduces the impact of OCR errors and query misspellings.},
  eventtitle = {{{ACM}}/{{IEEE-CS Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  langid = {english},
  file = {C:\Users\bdh19\Zotero\storage\J5ZE6Q63\Tran 等 - 2024 - Retrieval Augmented Generation for Historical News.pdf}
}

@online{xiao2025,
  title = {{{GraphRAG-Bench}}: {{Challenging Domain-Specific Reasoning}} for {{Evaluating Graph Retrieval-Augmented Generation}}},
  shorttitle = {{{GraphRAG-Bench}}},
  author = {Xiao, Yilin and Dong, Junnan and Zhou, Chuang and Dong, Su and Zhang, Qian-wen and Yin, Di and Sun, Xing and Huang, Xiao},
  date = {2025-06-11},
  eprint = {2506.02404},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2506.02404},
  url = {http://arxiv.org/abs/2506.02404},
  urldate = {2025-06-16},
  abstract = {Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing recognition for its potential to enhance large language models (LLMs) by structurally organizing domain-specific corpora and facilitating complex reasoning. However, current evaluations of GraphRAG models predominantly rely on traditional question-answering datasets. Their limited scope in questions and evaluation metrics fails to comprehensively assess the reasoning capacity improvements enabled by GraphRAG models. To address this gap, we introduce GraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously evaluate GraphRAG models. Our benchmark offers three key superiorities: \textbackslash ((i)\textbackslash ) Challenging question design. Featuring college-level, domain-specific questions that demand multi-hop reasoning, the benchmark ensures that simple content retrieval is insufficient for problem-solving. For example, some questions require mathematical reasoning or programming. \textbackslash ((ii)\textbackslash ) Diverse task coverage. The dataset includes a broad spectrum of reasoning tasks, multiple-choice, true/false, multi-select, open-ended, and fill-in-the-blank. It spans 16 disciplines in twenty core textbooks. \textbackslash ((iii)\textbackslash ) Holistic evaluation framework. GraphRAG-Bench provides comprehensive assessment across the entire GraphRAG pipeline, including graph construction, knowledge retrieval, and answer generation. Beyond final-answer correctness, it evaluates the logical coherence of the reasoning process. By applying nine contemporary GraphRAG methods to GraphRAG-Bench, we demonstrate its utility in quantifying how graph-based structuring improves model reasoning capabilities. Our analysis reveals critical insights about graph architectures, retrieval efficacy, and reasoning capabilities, offering actionable guidance for the research community.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\56EZ6A2G\\Xiao 等 - 2025 - GraphRAG-Bench Challenging Domain-Specific Reason.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\W993C4V4\\2506.html}
}

@online{zaratiana2023,
  title = {{{GLiNER}}: {{Generalist Model}} for {{Named Entity Recognition}} using {{Bidirectional Transformer}}},
  shorttitle = {{{GLiNER}}},
  author = {Zaratiana, Urchade and Tomeh, Nadi and Holat, Pierre and Charnois, Thierry},
  date = {2023-11-14},
  eprint = {2311.08526},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2311.08526},
  url = {http://arxiv.org/abs/2311.08526},
  urldate = {2024-11-18},
  abstract = {Named Entity Recognition (NER) is essential in various Natural Language Processing (NLP) applications. Traditional NER models are effective but limited to a set of predefined entity types. In contrast, Large Language Models (LLMs) can extract arbitrary entities through natural language instructions, offering greater flexibility. However, their size and cost, particularly for those accessed via APIs like ChatGPT, make them impractical in resource-limited scenarios. In this paper, we introduce a compact NER model trained to identify any type of entity. Leveraging a bidirectional transformer encoder, our model, GLiNER, facilitates parallel entity extraction, an advantage over the slow sequential token generation of LLMs. Through comprehensive testing, GLiNER demonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs in zero-shot evaluations on various NER benchmarks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\E6NKH3E3\\Zaratiana 等 - 2023 - GLiNER Generalist Model for Named Entity Recognit.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\Y79B4XXP\\2311.html}
}

@book{zotero-515,
  title = {Histoire du Parlement. De 1789 à nos jours - Jean Garrigues},
  url = {https://www.decitre.fr/livres/histoire-du-parlement-9782200350352.html},
  urldate = {2025-04-14},
  abstract = {Pourquoi une histoire du Parlement français depuis 1789 ? Tout d'abord parce que cette histoire globale du parlementarisme n'existait pas. Aussi étonnant que cela puisse paraître, aucune synthèse n'a été réalisée jusqu'à ce jour sur cet élément central de notre histoire politique qu'est le Parle...},
  langid = {french},
  file = {C:\Users\bdh19\Zotero\storage\J2NQHLXJ\histoire-du-parlement-9782200350352.html}
}

@article{chikvaidze2020,
  title = {Multilateralism: {{Its Past}}, {{Present}} and {{Future}}},
  author = {Chikvaidze, David A},
  date = {2020},
  volume = {4},
  number = {2},
  abstract = {The narrative ‘sweeps through’ history, starting with the Treaties of Westphalia in 1648, on to the Congress of Vienna of 1814-15, to the current terminology of ‘modern multilateralism’ with its lineage from the Versailles Treaty of 1919 and the League of Nations, to the United Nations Monetary and Financial Conference at Bretton Woods in 1944, the European Coal and Steel Community of 1950, to the Nuclear Non-Proliferation Treaty (NPT) of 1970 and concluding the sweep with the Helsinki Process culminating in 1975. The objective of the ‘sweep through history’ and its main thrust is to analyze how at different times, the world powers of the day turned to multilateralism only after some prolonged, devastating conflict that they had had either blundered, or charged into, left them with no choice, but to sit down and talk, negotiate and take into account a balance of the interests of all parties. All these build up to a point where the narrative explores today’s challenges and ‘attacks’ on multilateralism and the seeming inability of the international community to reengage and work together, to stem, in the words of the United Nations Secretary-General “the wind of madness sweeping the globe.” The article makes the case, essentially, for the obvious: we are on the verge of blundering into something far more devastating than the world has experienced before for a variety of reasons, not least among them, unusually deteriorated relations among the most heavily armed and powerful States, a climate catastrophe that is already at our doorstep, the dark side of the unprecedented, quantum leaps in technological development, the deficit of trust among peoples, countries, communities and societies. Add to that the ‘game-changing’ COVID-19 pandemic and what the world has before it, is a stage set for planetary calamity. We should pull back from the precipice in time. Multilateralism, modern multilateralism, which marks its 100th anniversary this year, is the only way to do this.},
  langid = {english},
  file = {C:\Users\bdh19\Downloads\Multilateralism Its Past Present and Future.pdf}
}

@online{library,
  title = {Research {{Guides}}: {{Multilateralism}}: {{About History}} of {{Multilateralism}}},
  shorttitle = {Research {{Guides}}},
  author = {Library, UN Geneva},
  url = {https://libraryresources.unog.ch/multilateralism/history},
  urldate = {2025-06-15},
  abstract = {Research Guides: Multilateralism: About History of Multilateralism},
  langid = {english},
  file = {C:\Users\bdh19\Zotero\storage\2HRRDCTB\history.html}
}

@online{nations,
  title = {Multilateral {{System}}},
  author = {Nations, United},
  publisher = {United Nations},
  url = {https://www.un.org/en/global-issues/multilateral-system},
  urldate = {2025-06-15},
  abstract = {Multilateralism helps nations to confront complex global challenges through a universal approach. Standing at the heart of multilateralism, the United Nations forms the backbone of the contemporary multilateral system, serving as a platform for dialogue, cooperation, and collective action. \&nbsp;},
  langid = {english},
  organization = {United Nations}
}

@online{nationsa,
  title = {Charte des Nations Unies (version intégrale) | Nations Unies},
  author = {Nations, United},
  publisher = {United Nations},
  url = {https://www.un.org/fr/about-us/un-charter/full-text},
  urldate = {2025-07-14},
  langid = {french},
  organization = {United Nations},
  file = {C:\Users\bdh19\Zotero\storage\LHGUZ23K\full-text.html}
}

@incollection{petiteville2020,
  title = {Multilateral {{Diplomacy}}},
  booktitle = {Global {{Diplomacy}}: {{An Introduction}} to {{Theory}} and {{Practice}}},
  author = {Petiteville, Franck and Placidi-Frot, Delphine},
  editor = {Balzacq, Thierry and Charillon, Frédéric and Ramel, Frédéric},
  date = {2020},
  pages = {35--47},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-28786-3_3},
  url = {https://doi.org/10.1007/978-3-030-28786-3_3},
  urldate = {2025-06-12},
  abstract = {This chapter traces back the history of multilateral diplomacy since 1648, which first took the form of ad hoc conference diplomacy aimed at restoring peace after recurrent wars in Europe. It then shows that contemporary diplomacy has been institutionalized since the creation of the League of Nations and of the United Nations, leading states to adapt their diplomatic practices to the proliferation of international organizations. The chapter concludes by analyzing the features of contemporary multilateral negotiations: The high number of states and other actors involved, the technicality of issues at stake, the frequent research for consensus, and the added value of “soft leadership” and coalition building.},
  isbn = {978-3-030-28786-3},
  langid = {english},
  file = {C:\Users\bdh19\Zotero\storage\4UX7XI3C\Petiteville 和 Placidi-Frot - 2020 - Multilateral Diplomacy.pdf}
}

@article{ruggie1992,
  title = {Multilateralism: the {{Anatomy}} of an {{Institution}}},
  author = {Ruggie, John Gerard},
  date = {1992},
  journaltitle = {International Organization},
  volume = {46},
  number = {3},
  eprint = {2706989},
  eprinttype = {jstor},
  pages = {561--598},
  url = {http://www.jstor.org/stable/2706989},
  langid = {english},
  file = {C:\Users\bdh19\Zotero\storage\BDKLIQ4I\Ruggie - 1992 - Multilateralism the Anatomy of an Institution.pdf}
}

@incollection{telo2012,
  title = {State and {{Multilateralism}}: {{History}} and {{Perspectives}}},
  shorttitle = {State and {{Multilateralism}}},
  booktitle = {State, {{Globalization}} and {{Multilateralism}}: {{The}} challenges of institutionalizing regionalism},
  author = {Telò, Mario},
  editor = {Telò, Mario},
  date = {2012},
  pages = {7--44},
  publisher = {Springer Netherlands},
  location = {Dordrecht},
  doi = {10.1007/978-94-007-2843-1_2},
  url = {https://doi.org/10.1007/978-94-007-2843-1_2},
  urldate = {2025-06-12},
  abstract = {There is an evident gap between the increasing relevance of multilateral regimes, arrangements and organizations on the one hand and the existing multidisciplinary research on this crucial side of global governance and international life.},
  isbn = {978-94-007-2843-1},
  langid = {english}
}

@article{vanderwusten2011,
  title = {The {{Map}} of {{Multilateral Treaty-Making}} 1600–2000: {{A Contribution}} to the {{Historical Geography}} of {{Diplomacy}}},
  shorttitle = {The {{Map}} of {{Multilateral Treaty-Making}} 1600–2000},
  author = {Van Der Wusten, Herman and Denemark, Robert A. and Hoffmann, Matthew and Yonten, Hasan},
  date = {2011},
  journaltitle = {Tijdschrift voor Economische en Sociale Geografie},
  volume = {102},
  number = {5},
  pages = {499--514},
  issn = {1467-9663},
  doi = {10.1111/j.1467-9663.2011.00653.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9663.2011.00653.x},
  urldate = {2025-06-12},
  abstract = {The paper describes and analyses the successive geographical distributions of places where multilateral treaties have been signed over the life course of the state system. A large proportion of all negotiations occurred in just a few places and the collection of most frequently selected places shows considerable continuity over time. Treaty-making emerges as more of a secular trend than a cyclical pulse, being insignificantly impacted by economic cycles, and inconsistently impacted by hegemonic cycles. The work presents a measure of specialisation that helps to identify types of central venues in the multilateral treaty-making system. The actual selection of specific venues suggests functional and political considerations to have been most important. The sustained preference for national political centres expresses the importance of such considerations, while the actual choice of a venue in a specific case can be highly contingent.},
  langid = {english},
  keywords = {Diplomacy,hegemony,multilateral treaties,political centres,world-systems analysis},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\G6D6FZTM\\Van Der Wusten 等 - 2011 - The Map of Multilateral Treaty-Making 1600–2000 A.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\5KGLERZJ\\j.1467-9663.2011.00653.html}
}

@online{zotero-540,
  title = {League of {{Nations Archives}} - {{UN Archives Geneva}}},
  url = {https://archives.ungeneva.org/lontad},
  urldate = {2025-06-15},
  file = {C:\Users\bdh19\Zotero\storage\FNJFHUJS\lontad.html}
}

@article{eddy1996,
  title = {Hidden {{Markov}} models},
  author = {Eddy, Sean R},
  date = {1996-06-01},
  journaltitle = {Current Opinion in Structural Biology},
  shortjournal = {Current Opinion in Structural Biology},
  volume = {6},
  number = {3},
  pages = {361--365},
  issn = {0959-440X},
  doi = {10.1016/S0959-440X(96)80056-X},
  url = {https://www.sciencedirect.com/science/article/pii/S0959440X9680056X},
  urldate = {2025-07-20},
  abstract = {‘Profiles’ of protein structures and sequence alignments can detect subtle homologies. Profile analysis has been put on firmer mathematical ground by the introduction of hidden Markov model (HMM) methods. During the past year, applications of these powerful new HMM-based profiles have begun to appear in the fields of protein-structure prediction and large-scale genome-sequence analysis.},
  file = {C:\Users\bdh19\Zotero\storage\ZVIIPIXJ\S0959440X9680056X.html}
}

@article{ehrmannmaud2023,
  title = {Named {{Entity Recognition}} and {{Classification}} in {{Historical Documents}}: {{A Survey}}},
  shorttitle = {Named {{Entity Recognition}} and {{Classification}} in {{Historical Documents}}},
  author = {EhrmannMaud and HamdiAhmed and Linhares, PontesElvys and RomanelloMatteo and DoucetAntoine},
  date = {2023-09-14},
  journaltitle = {ACM Computing Surveys},
  publisher = {ACMPUB27New York, NY},
  doi = {10.1145/3604931},
  url = {https://dl.acm.org/doi/10.1145/3604931},
  urldate = {2025-07-19},
  abstract = {After decades of massive digitisation, an unprecedented number of historical documents are available in digital format, along with their machine-readable texts. While this represents a major step forward with respect to preservation and accessibility, it ...},
  langid = {english},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\G54GDK8I\\EhrmannMaud 等 - 2023 - Named Entity Recognition and Classification in His.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\LHGHVYRH\\3604931.html}
}

@article{fields2023,
  title = {Using named entity recognition and network analysis to distinguish personal networks from the social milieu in nineteenth-century {{Ottoman}}–{{Iraqi}} personal diaries},
  author = {Fields, Sam and Cole, Camille Lyans and Oei, Catherine and Chen, Annie T},
  date = {2023-04-01},
  journaltitle = {Digital Scholarship in the Humanities},
  shortjournal = {Digital Scholarship in the Humanities},
  volume = {38},
  number = {1},
  pages = {66--86},
  issn = {2055-7671},
  doi = {10.1093/llc/fqac047},
  url = {https://doi.org/10.1093/llc/fqac047},
  urldate = {2025-07-20},
  abstract = {The diaries of Joseph Mathia Svoboda capture over 40 years of trade on the Tigris, describing his daily life and regular journeys as a steamboat purser during the late nineteenth and early twentieth centuries, specifically between the cities of Basra and Baghdad. They offer a unique perspective on daily life, community structure, and social relations. However, with over 600 pages of transcribed material and many more diaries still in the process of being transcribed, it is difficult to track patterns and changes in Joseph Svoboda’s social relationships and daily life by way of reading and inference alone. This article employs natural language processing (NLP) and network analysis to facilitate study of Svoboda’s social interactions, as well as his observations of his broader social milieu. Inspection of the networks and accompanying visualizations showed that Svoboda’s close interactions were primarily with kin, but his position as a steamship purser gave him a unique vantage point to encounter a wide range of persons of diverse backgrounds. Additionally, decomposing networks by time illustrated how significant life events facilitated change in social interactions.},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\NGI43PFP\\Fields 等 - 2023 - Using named entity recognition and network analysi.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\ZXLBUHCG\\6658426.html}
}

@article{hearst1998,
  title = {Support vector machines},
  author = {Hearst, M.A. and Dumais, S.T. and Osuna, E. and Platt, J. and Scholkopf, B.},
  date = {1998-07},
  journaltitle = {IEEE Intelligent Systems and their Applications},
  volume = {13},
  number = {4},
  pages = {18--28},
  issn = {2374-9423},
  doi = {10.1109/5254.708428},
  url = {https://ieeexplore.ieee.org/abstract/document/708428},
  urldate = {2025-07-20},
  abstract = {My first exposure to Support Vector Machines came this spring when heard Sue Dumais present impressive results on text categorization using this analysis technique. This issue's collection of essays should help familiarize our readers with this interesting new racehorse in the Machine Learning stable. Bernhard Scholkopf, in an introductory overview, points out that a particular advantage of SVMs over other learning algorithms is that it can be analyzed theoretically using concepts from computational learning theory, and at the same time can achieve good performance when applied to real problems. Examples of these real-world applications are provided by Sue Dumais, who describes the aforementioned text-categorization problem, yielding the best results to date on the Reuters collection, and Edgar Osuna, who presents strong results on application to face detection. Our fourth author, John Platt, gives us a practical guide and a new technique for implementing the algorithm efficiently.},
  keywords = {Algorithm design and analysis,Character recognition,Kernel,Machine learning,Neural networks,Pattern recognition,Polynomials,Support vector machines,Training data,Web pages},
  file = {C:\Users\bdh19\Zotero\storage\PUV8WJ8H\708428.html}
}

@article{humbel2021,
  title = {Named-entity recognition for early modern textual documents: a review of capabilities and challenges with strategies for the future},
  shorttitle = {Named-entity recognition for early modern textual documents},
  author = {Humbel, Marco and Nyhan, Julianne and Vlachidis, Andreas and Sloan, Kim and Ortolja-Baird, Alexandra},
  date = {2021-10-11},
  journaltitle = {Journal of Documentation},
  shortjournal = {JD},
  volume = {77},
  number = {6},
  pages = {1223--1247},
  publisher = {Emerald},
  issn = {0022-0418},
  doi = {10.1108/jd-02-2021-0032},
  url = {https://www.emerald.com/insight/content/doi/10.1108/JD-02-2021-0032/full/html},
  urldate = {2025-07-20},
  abstract = {PurposeBy mapping-out the capabilities, challenges and limitations of named-entity recognition (NER), this article aims to synthesise the state of the art of NER in the context of the early modern research field and to inform discussions about the kind of resources, methods and directions that may be pursued to enrich the application of the technique going forward.Design/methodology/approachThrough an extensive literature review, this article maps out the current capabilities, challenges and limitations of NER and establishes the state of the art of the technique in the context of the early modern, digitally augmented research field. It also presents a new case study of NER research undertaken by Enlightenment Architectures: Sir Hans Sloane's Catalogues of his Collections (2016–2021), a Leverhulme funded research project and collaboration between the British Museum and University College London, with contributing expertise from the British Library and the Natural History Museum.FindingsCurrently, it is not possible to benchmark the capabilities of NER as applied to documents of the early modern period. The authors also draw attention to the situated nature of authority files, and current conceptualisations of NER, leading them to the conclusion that more robust reporting and critical analysis of NER approaches and findings is required.Research limitations/implicationsThis article examines NER as applied to early modern textual sources, which are mostly studied by Humanists. As addressed in this article, detailed reporting of NER processes and outcomes is not necessarily valued by the disciplines of the Humanities, with the result that it can be difficult to locate relevant data and metrics in project outputs. The authors have tried to mitigate this by contacting projects discussed in this paper directly, to further verify the details they report here.Practical implicationsThe authors suggest that a forum is needed where tools are evaluated according to community standards. Within the wider NER community, the MUC and ConLL corpora are used for such experimental set-ups and are accompanied by a conference series, and may be seen as a useful model for this. The ultimate nature of such a forum must be discussed with the whole research community of the early modern domain.Social implicationsNER is an algorithmic intervention that transforms data according to certain rules-, patterns- or training data and ultimately affects how the authors interpret the results. The creation, use and promotion of algorithmic technologies like NER is not a neutral process, and neither is their output A more critical understanding of the role and impact of NER on early modern documents and research and focalization of some of the data- and human-centric aspects of NER routines that are currently overlooked are called for in this paper.Originality/valueThis article presents a state of the art snapshot of NER, its applications and potential, in the context of early modern research. It also seeks to inform discussions about the kinds of resources, methods and directions that may be pursued to enrich the application of NER going forward. It draws attention to the situated nature of authority files, and current conceptualisations of NER, and concludes that more robust reporting of NER approaches and findings are urgently required. The Appendix sets out a comprehensive summary of digital tools and resources surveyed in this article.},
  langid = {english},
  file = {C:\Users\bdh19\Zotero\storage\CHYZS7EG\Humbel 等 - 2021 - Named-entity recognition for early modern textual .pdf}
}

@online{keraghel2024,
  title = {Recent {{Advances}} in {{Named Entity Recognition}}: {{A Comprehensive Survey}} and {{Comparative Study}}},
  shorttitle = {Recent {{Advances}} in {{Named Entity Recognition}}},
  author = {Keraghel, Imed and Morbieu, Stanislas and Nadif, Mohamed},
  date = {2024-12-20},
  eprint = {2401.10825},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.10825},
  url = {http://arxiv.org/abs/2401.10825},
  urldate = {2025-07-20},
  abstract = {Named Entity Recognition seeks to extract substrings within a text that name real-world objects and to determine their type (for example, whether they refer to persons or organizations). In this survey, we first present an overview of recent popular approaches, including advancements in Transformer-based methods and Large Language Models (LLMs) that have not had much coverage in other surveys. In addition, we discuss reinforcement learning and graph-based approaches, highlighting their role in enhancing NER performance. Second, we focus on methods designed for datasets with scarce annotations. Third, we evaluate the performance of the main NER implementations on a variety of datasets with differing characteristics (as regards their domain, their size, and their number of classes). We thus provide a deep comparison of algorithms that have never been considered together. Our experiments shed some light on how the characteristics of datasets affect the behavior of the methods we compare.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\9SW6MDFJ\\Keraghel 等 - 2024 - Recent Advances in Named Entity Recognition A Com.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\IIQV9AAE\\2401.html}
}

@article{lafferty,
  title = {Conditional {{Random Fields}}: {{Probabilistic Models}} for {{Segmenting}} and {{Labeling Sequence Data}}},
  author = {Lafferty, John and McCallum, Andrew and Pereira, Fernando},
  abstract = {We present conditional random fields, a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.},
  langid = {english},
  file = {C:\Users\bdh19\Zotero\storage\N4ULH45K\Lafferty 等 - Conditional Random Fields Probabilistic Models fo.pdf}
}

@article{li2024,
  title = {A method for named entity recognition in social media texts with syntactically enhanced multiscale feature fusion},
  author = {Li, Yuhan and Zhou, Yang and Hu, Xiaofei and Li, Qingxiang and Tian, Jiali},
  date = {2024-11-15},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {14},
  eprint = {39548167},
  eprinttype = {pmid},
  pages = {28216},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-78948-5},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11568171/},
  urldate = {2025-07-20},
  abstract = {Social media data are characterized by significant noise and non-standardization, thereby posing challenges for existing methods in recognizing named entities owing to the entity sparsity and insufficient semantic richness. Thus, to deal with these issues, this study proposes SEMFF-NER, a named entity recognition (NER) method in social media texts that integrates multi-scale features and syntactic information. First, global features are extracted using a Transformer-based encoder (XLNET) with embedded dependency syntactic relations to enhance semantic representation. Next, sliding windows of different lengths capture local features, which are input into a bi-directional long short-term memory (BiLSTM) to capture multi-level local features. Subsequently, the fusion-attention mechanism effectively integrates global contextual information with multiple local features to predict the optimal entity labels. Extensive experiments conducted on three datasets collected from English social media platforms (WNUT2016, WNUT2017, OntoNotes5.0\_English) demonstrate the advantageous performance of our proposed method, and ablation experiments further confirm the method’s viability and effectiveness.},
  pmcid = {PMC11568171}
}

@online{marcheggiani2017,
  title = {Encoding {{Sentences}} with {{Graph Convolutional Networks}} for {{Semantic Role Labeling}}},
  author = {Marcheggiani, Diego and Titov, Ivan},
  date = {2017-07-30},
  eprint = {1703.04826},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1703.04826},
  url = {http://arxiv.org/abs/1703.04826},
  urldate = {2025-07-20},
  abstract = {Semantic role labeling (SRL) is the task of identifying the predicate-argument structure of a sentence. It is typically regarded as an important step in the standard NLP pipeline. As the semantic representations are closely related to syntactic ones, we exploit syntactic information in our model. We propose a version of graph convolutional networks (GCNs), a recent class of neural networks operating on graphs, suited to model syntactic dependency graphs. GCNs over syntactic dependency trees are used as sentence encoders, producing latent feature representations of words in a sentence. We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009) both for Chinese and English.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\7PH25W5G\\Marcheggiani 和 Titov - 2017 - Encoding Sentences with Graph Convolutional Networ.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\MXHENSWW\\1703.html}
}

@inproceedings{tudor2024,
  title = {People and {{Places}} of the {{Past}} - {{Named Entity Recognition}} in {{Swedish Labour Movement Documents}} from {{Historical Sources}}},
  booktitle = {Proceedings of the 8th {{Joint SIGHUM Workshop}} on {{Computational Linguistics}} for {{Cultural Heritage}}, {{Social Sciences}}, {{Humanities}} and {{Literature}} ({{LaTeCH-CLfL}} 2024)},
  author = {Tudor, Crina and Pettersson, Eva},
  editor = {Bizzoni, Yuri and Degaetano-Ortlieb, Stefania and Kazantseva, Anna and Szpakowicz, Stan},
  date = {2024-03},
  pages = {185--195},
  publisher = {Association for Computational Linguistics},
  location = {St. Julians, Malta},
  url = {https://aclanthology.org/2024.latechclfl-1.17/},
  urldate = {2025-07-20},
  abstract = {Named Entity Recognition (NER) is an important step in many Natural Language Processing tasks. The existing state-of-the-art NER systems are however typically developed based on contemporary data, and not very well suited for analyzing historical text. In this paper, we present a comparative analysis of the performance of several language models when applied to Named Entity Recognition for historical Swedish text. The source texts we work with are documents from Swedish labour unions from the 19th and 20th century. We experiment with three off-the-shelf models for contemporary Swedish text, and one language model built on historical Swedish text that we fine-tune with labelled data for adaptation to the NER task. Lastly, we propose a hybrid approach by combining the results of two models in order to maximize usability. We show that, even though historical Swedish is a low-resource language with data sparsity issues affecting overall performance, historical language models still show very promising results. Further contributions of our paper are the release of our newly trained model for NER of historical Swedish text, along with a manually annotated corpus of over 650 named entities.},
  file = {C:\Users\bdh19\Zotero\storage\8DFLGTX7\Tudor 和 Pettersson - 2024 - People and Places of the Past - Named Entity Recog.pdf}
}

@inproceedings{wang2025,
  title = {{{GPT-NER}}: {{Named Entity Recognition}} via {{Large Language Models}}},
  shorttitle = {{{GPT-NER}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{NAACL}} 2025},
  author = {Wang, Shuhe and Sun, Xiaofei and Li, Xiaoya and Ouyang, Rongbin and Wu, Fei and Zhang, Tianwei and Li, Jiwei and Wang, Guoyin and Guo, Chen},
  editor = {Chiruzzo, Luis and Ritter, Alan and Wang, Lu},
  date = {2025-04},
  pages = {4257--4275},
  publisher = {Association for Computational Linguistics},
  location = {Albuquerque, New Mexico},
  doi = {10.18653/v1/2025.findings-naacl.239},
  url = {https://aclanthology.org/2025.findings-naacl.239/},
  urldate = {2025-07-20},
  abstract = {Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model.In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text “Columbus is a city” is transformed to generate the text sequence “@@Columbus\#\# is a city”, where special tokens @@\#\# marks the entity to extract. To efficiently address the hallucination issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag.We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.},
  eventtitle = {Findings 2025},
  isbn = {9798891761957},
  file = {C:\Users\bdh19\Zotero\storage\3A73QBDK\Wang 等 - 2025 - GPT-NER Named Entity Recognition via Large Langua.pdf}
}

@article{warto2024,
  title = {Systematic {{Literature Review}} on {{Named Entity Recognition}}: {{Approach}}, {{Method}}, and {{Application}}},
  shorttitle = {Systematic {{Literature Review}} on {{Named Entity Recognition}}},
  author = {Warto and Rustad, Supriadi and Shidik, Guruh Fajar and Noersasongko, Edi and Purwanto and Muljono and Setiadi, De Rosal Ignatius Moses},
  date = {2024-02-28},
  journaltitle = {Statistics, Optimization \& Information Computing},
  volume = {12},
  number = {4},
  pages = {907--942},
  issn = {2310-5070},
  doi = {10.19139/soic-2310-5070-1631},
  url = {http://iapress.org/index.php/soic/article/view/1631},
  urldate = {2025-07-20},
  abstract = {Named entity recognition (NER) is one of the preprocessing stages in natural language processing (NLP), which functions to detect and classify entities in the corpus. NER results are used in various NLP applications, including sentiment analysis, text summarization, chatbot, machine translation, and question answering. Several previous reviews partially discussed NER, for instance, NER reviews in specific domains, NER classification, and NER deep learning. This paper provides a comprehensive and systematic review on NER topic studies published from 2011 to 2020. The main contribution of this review is to present a comprehensive systematic literature review on NER from preprocessing techniques, datasets, application domains, feature extraction techniques, approaches, methods, and evaluation techniques. The result concludes that the deep learning approach and the Bi-directional long short-term memory with a conditional random field (Bi-LSTM-CRF) method are the most interesting methods among NER researchers. At the same time, medical and health are NER researchers' most popular domains. These developments have also led to an increasing number of public datasets in the medical and health fields. At the end of this review, we recommend some opportunities and challenges for NER research going forward.},
  issue = {4},
  langid = {english},
  keywords = {entity classification,entity detection,entity extraction,Named entity recognition,natural language processing},
  file = {C:\Users\bdh19\Zotero\storage\WWBLISQU\Warto 等 - 2024 - Systematic Literature Review on Named Entity Recog.pdf}
}

@article{yang2024,
  title = {Evolution and emerging trends of named entity recognition: {{Bibliometric}} analysis from 2000 to 2023},
  shorttitle = {Evolution and emerging trends of named entity recognition},
  author = {Yang, Jun and Zhang, Taihua and Tsai, Chieh-Yuan and Lu, Yao and Yao, Liguo},
  date = {2024-04-22},
  journaltitle = {Heliyon},
  shortjournal = {Heliyon},
  volume = {10},
  number = {9},
  eprint = {38707358},
  eprinttype = {pmid},
  pages = {e30053},
  issn = {2405-8440},
  doi = {10.1016/j.heliyon.2024.e30053},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11066397/},
  urldate = {2025-07-20},
  abstract = {Identifying valuable information within the extensive texts documented in natural language presents a significant challenge in various disciplines. Named Entity Recognition (NER), as one of the critical technologies in text data processing and mining, has become a current research hotspot. To accurately and objectively review the progress in NER, this paper employs bibliometric methods. It analyzes 1300 documents related to NER obtained from the Web of Science database using CiteSpace software. Firstly, statistical analysis is performed on the literature and journals that were obtained to explore the distribution characteristics of the literature. Secondly, the core authors in the field of NER, the development of the technology in different countries, and the leading institutions are explored by analyzing the number of publications and the cooperation network graph. Finally, explore the research frontiers, development tracks, research hotspots, and other information in this field from a scientific point of view, and further discuss the five research frontiers and seven research hotspots in depth. This paper explores the progress of NER research from both macro and micro perspectives. It aims to assist researchers in quickly grasping relevant information and offers constructive ideas and suggestions to promote the development of NER., •A bibliometric approach to analyzing hotspots and frontiers in the field of NER.•Explosive document analysis explores mainstream approaches in various periods of NER.•Explore the cooperation and publication among authors, institutions, and countries.•Statistical methods explore the distributional characteristics of literature.},
  pmcid = {PMC11066397}
}

@online{zotero-664,
  title = {Linguistic {{Features}} · {{spaCy Usage Documentation}}},
  url = {https://spacy.io/usage/linguistic-features#accessing-ner},
  urldate = {2025-07-20},
  abstract = {spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more.},
  langid = {english},
  organization = {Linguistic Features},
  file = {C:\Users\bdh19\Zotero\storage\ALEJC26D\linguistic-features.html}
}

@online{vaswani2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-02},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2025-07-20},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\Y548MALV\\Vaswani 等 - 2023 - Attention Is All You Need.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\BRCZHMXY\\1706.html}
}

@article{ciletti,
  title = {Retrieval-{{Augmented Generation}} systems for enhanced access to digital archives},
  author = {Ciletti, Michele},
  abstract = {ENGLISH) This study investigates the potential of Retrieval-Augmented Generation (RAG) to enhance access to digital archives related to humanities topics, using a case study on historical newspaper data. Over the past years, Large Language Models (LLMs) have shown remarkable capabilities in generating natural language; however, they still suffer from hallucinations and context window limitations. As a solution, RAG frameworks have emerged, linking a language model to external databases in order to provide grounded, reliable sources. In this work, an experimental RAG pipeline was tested on the “Foggia Occupator Dataset”, which includes articles from a 1945–1946 periodical published by the US forces occupying Foggia, Italy, at the end of World War II. To assess performance, domain experts constructed a set of ten questions on topics such as historical figures, examined events, and stylistic aspects of the articles. The corresponding “ground truth” answers were used for both quantitative and qualitative evaluations. Results show that the RAG system performs well in precisely defined scenarios, retrieving accurate information and correctly identifying named entities. However, broader questions occasionally led to incomplete or slightly erroneous answers, pointing to potential areas for algorithmic refinement—particularly in optimizing retrieval for complex or multi-article queries. The study concludes that RAG can significantly improve the searchability and reliability of digital archives, but continued improvements in metadata enrichment, query strategies, and retrieval algorithms are needed.},
  langid = {english},
  file = {C:\Users\bdh19\Zotero\storage\PGUTGTB8\Ciletti - Retrieval-Augmented Generation systems for enhance.pdf}
}

@online{gutierrez2025c,
  title = {From {{RAG}} to {{Memory}}: {{Non-Parametric Continual Learning}} for {{Large Language Models}}},
  shorttitle = {From {{RAG}} to {{Memory}}},
  author = {Gutiérrez, Bernal Jiménez and Shu, Yiheng and Qi, Weijian and Zhou, Sizhe and Su, Yu},
  date = {2025-06-19},
  eprint = {2502.14802},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.14802},
  url = {http://arxiv.org/abs/2502.14802},
  urldate = {2025-07-21},
  abstract = {Our ability to continuously acquire, organize, and leverage knowledge is a key feature of human intelligence that AI systems must approximate to unlock their full potential. Given the challenges in continual learning with large language models (LLMs), retrieval-augmented generation (RAG) has become the dominant way to introduce new information. However, its reliance on vector retrieval hinders its ability to mimic the dynamic and interconnected nature of human long-term memory. Recent RAG approaches augment vector embeddings with various structures like knowledge graphs to address some of these gaps, namely sense-making and associativity. However, their performance on more basic factual memory tasks drops considerably below standard RAG. We address this unintended deterioration and propose HippoRAG 2, a framework that outperforms standard RAG comprehensively on factual, sense-making, and associative memory tasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in HippoRAG and enhances it with deeper passage integration and more effective online use of an LLM. This combination pushes this RAG system closer to the effectiveness of human long-term memory, achieving a 7\% improvement in associative memory tasks over the state-of-the-art embedding model while also exhibiting superior factual knowledge and sense-making memory capabilities. This work paves the way for non-parametric continual learning for LLMs. Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\IHCSSVHI\\Gutiérrez 等 - 2025 - From RAG to Memory Non-Parametric Continual Learn.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\GKBL44AN\\2502.html}
}

@online{gutierrez2025d,
  title = {{{HippoRAG}}: {{Neurobiologically Inspired Long-Term Memory}} for {{Large Language Models}}},
  shorttitle = {{{HippoRAG}}},
  author = {Gutiérrez, Bernal Jiménez and Shu, Yiheng and Gu, Yu and Yasunaga, Michihiro and Su, Yu},
  date = {2025-01-14},
  eprint = {2405.14831},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.14831},
  url = {http://arxiv.org/abs/2405.14831},
  urldate = {2025-07-21},
  abstract = {In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting. Despite the impressive accomplishments, large language models (LLMs), even with retrieval-augmented generation (RAG), still struggle to efficiently and effectively integrate a large amount of new experiences after pre-training. In this work, we introduce HippoRAG, a novel retrieval framework inspired by the hippocampal indexing theory of human long-term memory to enable deeper and more efficient knowledge integration over new experiences. HippoRAG synergistically orchestrates LLMs, knowledge graphs, and the Personalized PageRank algorithm to mimic the different roles of neocortex and hippocampus in human memory. We compare HippoRAG with existing RAG methods on multi-hop question answering and show that our method outperforms the state-of-the-art methods remarkably, by up to 20\%. Single-step retrieval with HippoRAG achieves comparable or better performance than iterative retrieval like IRCoT while being 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into IRCoT brings further substantial gains. Finally, we show that our method can tackle new types of scenarios that are out of reach of existing methods. Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\54U5UEW5\\Gutiérrez 等 - 2025 - HippoRAG Neurobiologically Inspired Long-Term Mem.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\PN39W8HM\\2405.html}
}

@online{wang2024a,
  title = {Multilingual {{E5 Text Embeddings}}: {{A Technical Report}}},
  shorttitle = {Multilingual {{E5 Text Embeddings}}},
  author = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  date = {2024-02-08},
  eprint = {2402.05672},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.05672},
  url = {http://arxiv.org/abs/2402.05672},
  urldate = {2025-07-21},
  abstract = {This technical report presents the training methodology and evaluation results of the open-source multilingual E5 text embedding models, released in mid-2023. Three embedding models of different sizes (small / base / large) are provided, offering a balance between the inference efficiency and embedding quality. The training procedure adheres to the English E5 model recipe, involving contrastive pre-training on 1 billion multilingual text pairs, followed by fine-tuning on a combination of labeled datasets. Additionally, we introduce a new instruction-tuned embedding model, whose performance is on par with state-of-the-art, English-only models of similar sizes. Information regarding the model release can be found at https://github.com/microsoft/unilm/tree/master/e5 .},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\TBW5PI2M\\Wang 等 - 2024 - Multilingual E5 Text Embeddings A Technical Repor.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\XCDIIZ63\\2402.html}
}

@online{xiang2025,
  title = {When to use {{Graphs}} in {{RAG}}: {{A Comprehensive Analysis}} for {{Graph Retrieval-Augmented Generation}}},
  shorttitle = {When to use {{Graphs}} in {{RAG}}},
  author = {Xiang, Zhishang and Wu, Chuanjie and Zhang, Qinggang and Chen, Shengyuan and Hong, Zijin and Huang, Xiao and Su, Jinsong},
  date = {2025-06-06},
  eprint = {2506.05690},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2506.05690},
  url = {http://arxiv.org/abs/2506.05690},
  urldate = {2025-07-20},
  abstract = {Graph retrieval-augmented generation (GraphRAG) has emerged as a powerful paradigm for enhancing large language models (LLMs) with external knowledge. It leverages graphs to model the hierarchical structure between specific concepts, enabling more coherent and effective knowledge retrieval for accurate reasoning.Despite its conceptual promise, recent studies report that GraphRAG frequently underperforms vanilla RAG on many real-world tasks. This raises a critical question: Is GraphRAG really effective, and in which scenarios do graph structures provide measurable benefits for RAG systems? To address this, we propose GraphRAG-Bench, a comprehensive benchmark designed to evaluate GraphRAG models onboth hierarchical knowledge retrieval and deep contextual reasoning. GraphRAG-Bench features a comprehensive dataset with tasks of increasing difficulty, coveringfact retrieval, complex reasoning, contextual summarization, and creative generation, and a systematic evaluation across the entire pipeline, from graph constructionand knowledge retrieval to final generation. Leveraging this novel benchmark, we systematically investigate the conditions when GraphRAG surpasses traditional RAG and the underlying reasons for its success, offering guidelines for its practical application. All related resources and analyses are collected for the community at https://github.com/GraphRAG-Bench/GraphRAG-Benchmark.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\JW7T3KNQ\\Xiang 等 - 2025 - When to use Graphs in RAG A Comprehensive Analysi.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\YMNUBCGU\\2506.html}
}

@online{openai2024,
  title = {{{GPT-4o System Card}}},
  author = {OpenAI and Hurst, Aaron and Lerer, Adam and Goucher, Adam P. and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, A. J. and Welihinda, Akila and Hayes, Alan and Radford, Alec and Mądry, Aleksander and Baker-Whitcomb, Alex and Beutel, Alex and Borzunov, Alex and Carney, Alex and Chow, Alex and Kirillov, Alex and Nichol, Alex and Paino, Alex and Renzin, Alex and Passos, Alex Tachard and Kirillov, Alexander and Christakis, Alexi and Conneau, Alexis and Kamali, Ali and Jabri, Allan and Moyer, Allison and Tam, Allison and Crookes, Amadou and Tootoochian, Amin and Tootoonchian, Amin and Kumar, Ananya and Vallone, Andrea and Karpathy, Andrej and Braunstein, Andrew and Cann, Andrew and Codispoti, Andrew and Galu, Andrew and Kondrich, Andrew and Tulloch, Andrew and Mishchenko, Andrey and Baek, Angela and Jiang, Angela and Pelisse, Antoine and Woodford, Antonia and Gosalia, Anuj and Dhar, Arka and Pantuliano, Ashley and Nayak, Avi and Oliver, Avital and Zoph, Barret and Ghorbani, Behrooz and Leimberger, Ben and Rossen, Ben and Sokolowsky, Ben and Wang, Ben and Zweig, Benjamin and Hoover, Beth and Samic, Blake and McGrew, Bob and Spero, Bobby and Giertler, Bogo and Cheng, Bowen and Lightcap, Brad and Walkin, Brandon and Quinn, Brendan and Guarraci, Brian and Hsu, Brian and Kellogg, Bright and Eastman, Brydon and Lugaresi, Camillo and Wainwright, Carroll and Bassin, Cary and Hudson, Cary and Chu, Casey and Nelson, Chad and Li, Chak and Shern, Chan Jun and Conger, Channing and Barette, Charlotte and Voss, Chelsea and Ding, Chen and Lu, Cheng and Zhang, Chong and Beaumont, Chris and Hallacy, Chris and Koch, Chris and Gibson, Christian and Kim, Christina and Choi, Christine and McLeavey, Christine and Hesse, Christopher and Fischer, Claudia and Winter, Clemens and Czarnecki, Coley and Jarvis, Colin and Wei, Colin and Koumouzelis, Constantin and Sherburn, Dane and Kappler, Daniel and Levin, Daniel and Levy, Daniel and Carr, David and Farhi, David and Mely, David and Robinson, David and Sasaki, David and Jin, Denny and Valladares, Dev and Tsipras, Dimitris and Li, Doug and Nguyen, Duc Phong and Findlay, Duncan and Oiwoh, Edede and Wong, Edmund and Asdar, Ehsan and Proehl, Elizabeth and Yang, Elizabeth and Antonow, Eric and Kramer, Eric and Peterson, Eric and Sigler, Eric and Wallace, Eric and Brevdo, Eugene and Mays, Evan and Khorasani, Farzad and Such, Felipe Petroski and Raso, Filippo and Zhang, Francis and family=Lohmann, given=Fred, prefix=von, useprefix=false and Sulit, Freddie and Goh, Gabriel and Oden, Gene and Salmon, Geoff and Starace, Giulio and Brockman, Greg and Salman, Hadi and Bao, Haiming and Hu, Haitang and Wong, Hannah and Wang, Haoyu and Schmidt, Heather and Whitney, Heather and Jun, Heewoo and Kirchner, Hendrik and Pinto, Henrique Ponde de Oliveira and Ren, Hongyu and Chang, Huiwen and Chung, Hyung Won and Kivlichan, Ian and O'Connell, Ian and O'Connell, Ian and Osband, Ian and Silber, Ian and Sohl, Ian and Okuyucu, Ibrahim and Lan, Ikai and Kostrikov, Ilya and Sutskever, Ilya and Kanitscheider, Ingmar and Gulrajani, Ishaan and Coxon, Jacob and Menick, Jacob and Pachocki, Jakub and Aung, James and Betker, James and Crooks, James and Lennon, James and Kiros, Jamie and Leike, Jan and Park, Jane and Kwon, Jason and Phang, Jason and Teplitz, Jason and Wei, Jason and Wolfe, Jason and Chen, Jay and Harris, Jeff and Varavva, Jenia and Lee, Jessica Gan and Shieh, Jessica and Lin, Ji and Yu, Jiahui and Weng, Jiayi and Tang, Jie and Yu, Jieqi and Jang, Joanne and Candela, Joaquin Quinonero and Beutler, Joe and Landers, Joe and Parish, Joel and Heidecke, Johannes and Schulman, John and Lachman, Jonathan and McKay, Jonathan and Uesato, Jonathan and Ward, Jonathan and Kim, Jong Wook and Huizinga, Joost and Sitkin, Jordan and Kraaijeveld, Jos and Gross, Josh and Kaplan, Josh and Snyder, Josh and Achiam, Joshua and Jiao, Joy and Lee, Joyce and Zhuang, Juntang and Harriman, Justyn and Fricke, Kai and Hayashi, Kai and Singhal, Karan and Shi, Katy and Karthik, Kavin and Wood, Kayla and Rimbach, Kendra and Hsu, Kenny and Nguyen, Kenny and Gu-Lemberg, Keren and Button, Kevin and Liu, Kevin and Howe, Kiel and Muthukumar, Krithika and Luther, Kyle and Ahmad, Lama and Kai, Larry and Itow, Lauren and Workman, Lauren and Pathak, Leher and Chen, Leo and Jing, Li and Guy, Lia and Fedus, Liam and Zhou, Liang and Mamitsuka, Lien and Weng, Lilian and McCallum, Lindsay and Held, Lindsey and Ouyang, Long and Feuvrier, Louis and Zhang, Lu and Kondraciuk, Lukas and Kaiser, Lukasz and Hewitt, Luke and Metz, Luke and Doshi, Lyric and Aflak, Mada and Simens, Maddie and Boyd, Madelaine and Thompson, Madeleine and Dukhan, Marat and Chen, Mark and Gray, Mark and Hudnall, Mark and Zhang, Marvin and Aljubeh, Marwan and Litwin, Mateusz and Zeng, Matthew and Johnson, Max and Shetty, Maya and Gupta, Mayank and Shah, Meghan and Yatbaz, Mehmet and Yang, Meng Jia and Zhong, Mengchao and Glaese, Mia and Chen, Mianna and Janner, Michael and Lampe, Michael and Petrov, Michael and Wu, Michael and Wang, Michele and Fradin, Michelle and Pokrass, Michelle and Castro, Miguel and family=Castro, given=Miguel Oom Temudo, prefix=de, useprefix=false and Pavlov, Mikhail and Brundage, Miles and Wang, Miles and Khan, Minal and Murati, Mira and Bavarian, Mo and Lin, Molly and Yesildal, Murat and Soto, Nacho and Gimelshein, Natalia and Cone, Natalie and Staudacher, Natalie and Summers, Natalie and LaFontaine, Natan and Chowdhury, Neil and Ryder, Nick and Stathas, Nick and Turley, Nick and Tezak, Nik and Felix, Niko and Kudige, Nithanth and Keskar, Nitish and Deutsch, Noah and Bundick, Noel and Puckett, Nora and Nachum, Ofir and Okelola, Ola and Boiko, Oleg and Murk, Oleg and Jaffe, Oliver and Watkins, Olivia and Godement, Olivier and Campbell-Moore, Owen and Chao, Patrick and McMillan, Paul and Belov, Pavel and Su, Peng and Bak, Peter and Bakkum, Peter and Deng, Peter and Dolan, Peter and Hoeschele, Peter and Welinder, Peter and Tillet, Phil and Pronin, Philip and Tillet, Philippe and Dhariwal, Prafulla and Yuan, Qiming and Dias, Rachel and Lim, Rachel and Arora, Rahul and Troll, Rajan and Lin, Randall and Lopes, Rapha Gontijo and Puri, Raul and Miyara, Reah and Leike, Reimar and Gaubert, Renaud and Zamani, Reza and Wang, Ricky and Donnelly, Rob and Honsby, Rob and Smith, Rocky and Sahai, Rohan and Ramchandani, Rohit and Huet, Romain and Carmichael, Rory and Zellers, Rowan and Chen, Roy and Chen, Ruby and Nigmatullin, Ruslan and Cheu, Ryan and Jain, Saachi and Altman, Sam and Schoenholz, Sam and Toizer, Sam and Miserendino, Samuel and Agarwal, Sandhini and Culver, Sara and Ethersmith, Scott and Gray, Scott and Grove, Sean and Metzger, Sean and Hermani, Shamez and Jain, Shantanu and Zhao, Shengjia and Wu, Sherwin and Jomoto, Shino and Wu, Shirong and Shuaiqi and Xia and Phene, Sonia and Papay, Spencer and Narayanan, Srinivas and Coffey, Steve and Lee, Steve and Hall, Stewart and Balaji, Suchir and Broda, Tal and Stramer, Tal and Xu, Tao and Gogineni, Tarun and Christianson, Taya and Sanders, Ted and Patwardhan, Tejal and Cunninghman, Thomas and Degry, Thomas and Dimson, Thomas and Raoux, Thomas and Shadwell, Thomas and Zheng, Tianhao and Underwood, Todd and Markov, Todor and Sherbakov, Toki and Rubin, Tom and Stasi, Tom and Kaftan, Tomer and Heywood, Tristan and Peterson, Troy and Walters, Tyce and Eloundou, Tyna and Qi, Valerie and Moeller, Veit and Monaco, Vinnie and Kuo, Vishal and Fomenko, Vlad and Chang, Wayne and Zheng, Weiyi and Zhou, Wenda and Manassra, Wesam and Sheu, Will and Zaremba, Wojciech and Patil, Yash and Qian, Yilei and Kim, Yongjik and Cheng, Youlong and Zhang, Yu and He, Yuchen and Zhang, Yuchen and Jin, Yujia and Dai, Yunxing and Malkov, Yury},
  date = {2024-10-25},
  eprint = {2410.21276},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.21276},
  url = {http://arxiv.org/abs/2410.21276},
  urldate = {2025-07-21},
  abstract = {GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50\textbackslash\% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House, we are sharing the GPT-4o System Card, which includes our Preparedness Framework evaluations. In this System Card, we provide a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures we've implemented to ensure the model is safe and aligned. We also include third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o's text and vision capabilities.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Computers and Society,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\ST7MT9AF\\OpenAI 等 - 2024 - GPT-4o System Card.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\D5L96V3E\\2410.html}
}

@online{wang2024,
  title = {Social-{{RAG}}: {{Retrieving}} from {{Group Interactions}} to {{Socially Ground Proactive AI Generation}} to {{Group Preferences}}},
  shorttitle = {Social-{{RAG}}},
  author = {Wang, Ruotong and Zhou, Xinyi and Qiu, Lin and Chang, Joseph Chee and Bragg, Jonathan and Zhang, Amy X.},
  date = {2024-11-04},
  eprint = {2411.02353},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2411.02353},
  url = {http://arxiv.org/abs/2411.02353},
  urldate = {2025-07-20},
  abstract = {AI agents are increasingly tasked with making proactive suggestions in online spaces where groups collaborate, but can be unhelpful or even annoying, due to not fitting the group's preferences or behaving in socially inappropriate ways. Fortunately, group spaces have a rich history of prior social interactions and affordances for social feedback to support creating agents that align to a group's interests and norms. We present Social-RAG, a workflow for grounding agents to social information about a group, which retrieves from prior group interactions, selects relevant social signals, and then feeds the context into a large language model to generate messages to the group. We implement this into PaperPing, our system that posts academic paper recommendations in group chat, leveraging social signals determined from formative studies with 39 researchers. From a three-month deployment in 18 channels, we observed PaperPing posted relevant messages in groups without disrupting their existing social practices, fostering group common ground.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\652MSTL4\\Wang 等 - 2024 - Social-RAG Retrieving from Group Interactions to .pdf;C\:\\Users\\bdh19\\Zotero\\storage\\QLHVWW6N\\2411.html}
}

@online{15:31:00+00:00,
  title = {Causal {{Language Models}} in {{NLP}}},
  year = {15:31:00+00:00},
  url = {https://www.geeksforgeeks.org/nlp/causal-language-models-in-nlp/},
  urldate = {2025-07-21},
  abstract = {Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.},
  langid = {english},
  organization = {GeeksforGeeks},
  file = {C:\Users\bdh19\Zotero\storage\L7VTHEVF\causal-language-models-in-nlp.html}
}

@article{cigliano,
  title = {The {{Impact}} of {{Digital Analysis}} and {{Large Language Models}} in {{Digital Humanity}}},
  author = {Cigliano, Andrea and Fallucchi, Francesca and Gerardi, Marco},
  abstract = {The advent of digital analysis tools and Large Language Models (LLMs) has significantly altered the landscape of digital humanities, introducing new methodologies for processing and interpreting vast amounts of data. This paper provides a comprehensive analysis of these technologies, examining their implications for research within digital humanities. We focus on the transformative effects of digital analysis tools and LLMs, assessing their potential to enhance understanding and accessibility of complex humanistic data, while also discussing the inherent challenges and ethical considerations these technologies introduce. By analyzing case studies and reviewing recent developments in the field, this study aims to provide a nuanced understanding of how digital analysis and LLMs is reshaping scholarly practices in humanities disciplines.},
  langid = {english},
  file = {C:\Users\bdh19\Zotero\storage\AXQHBFBX\Cigliano 等 - The Impact of Digital Analysis and Large Language .pdf}
}

@article{dunivin2025,
  title = {Scaling hermeneutics: a guide to qualitative coding with {{LLMs}} for reflexive content analysis},
  shorttitle = {Scaling hermeneutics},
  author = {Dunivin, Zackary Okun},
  date = {2025-12},
  journaltitle = {EPJ Data Science},
  shortjournal = {EPJ Data Sci.},
  volume = {14},
  number = {1},
  pages = {1--22},
  publisher = {SpringerOpen},
  issn = {2193-1127},
  doi = {10.1140/epjds/s13688-025-00548-8},
  url = {https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-025-00548-8},
  urldate = {2025-07-20},
  abstract = {Qualitative coding, or content analysis, is more than just labeling text: it is a reflexive interpretive practice that shapes research questions, refines theoretical insights, and illuminates subtle social dynamics. As large language models (LLMs) become increasingly adept at nuanced language tasks, questions arise about whether—and how—they can assist in large-scale coding without eroding the interpretive depth that distinguishes qualitative analysis from traditional machine learning and other quantitative approaches to natural language processing. In this paper, we present a hybrid approach that preserves hermeneutic value while incorporating LLMs to scale the application of codes to large data sets that are impractical for manual coding. Our workflow retains the traditional cycle of codebook development and refinement, adding an iterative step to adapt definitions for machine comprehension, before ultimately replacing manual with automated text categorization. We demonstrate how to rewrite code descriptions for LLM-interpretation, as well as how structured prompts and prompting the model to explain its coding decisions (chain-of-thought) can substantially improve fidelity. Empirically, our case study of socio-historical codes highlights the promise of frontier AI language models to reliably interpret paragraph-long passages representative of a humanistic study. Throughout, we emphasize ethical and practical considerations, preserving space for critical reflection, and the ongoing need for human researchers’ interpretive leadership. These strategies can guide both traditional and computational scholars aiming to harness automation effectively and responsibly—maintaining the creative, reflexive rigor of qualitative coding while capitalizing on the efficiency afforded by LLMs.},
  issue = {1},
  langid = {english},
  file = {C:\Users\bdh19\Zotero\storage\RTEGD4CF\Dunivin - 2025 - Scaling hermeneutics a guide to qualitative codin.pdf}
}

@online{grattafiori2024,
  title = {The {{Llama}} 3 {{Herd}} of {{Models}}},
  author = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Wyatt, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Guzmán, Francisco and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Thattai, Govind and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Zhang, Jack and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and family=Linde, given=Jelmer, prefix=van der, useprefix=false and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Prasad, Karthik and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Lakhotia, Kushal and Rantala-Yeary, Lauren and family=Maaten, given=Laurens, prefix=van der, useprefix=false and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and family=Oliveira, given=Luke, prefix=de, useprefix=false and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Tsimpoukelli, Maria and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Zhang, Ning and Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Maheswari, Rohan and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Albiero, Vítor and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Wang, Xiaofang and Tan, Xiaoqing Ellen and Xia, Xide and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Srivastava, Aayushi and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Teo, Amos and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Dong, Annie and Franco, Annie and Goyal, Anuj and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and Paola, Beto De and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Liu, Ce and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Gao, Cynthia and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Le, Eric-Tuan and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Kokkinos, Filippos and Ozgenel, Firat and Caggioni, Francesco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Inan, Hakan and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Zhan, Hongyuan and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Leontiadis, Ilias and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Lam, Janice and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Jagadeesh, Kiran and Huang, Kun and Chawla, Kunal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Liu, Miao and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Mehta, Nikhil and Laptev, Nikolay Pavlovich and Dong, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Parthasarathy, Rangaprabhu and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Howes, Russ and Rinott, Ruty and Mehta, Sachin and Siby, Sachin and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Mahajan, Saurabh and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Patil, Shishir and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Deng, Summer and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Koehler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wu, Xiaojian and Wang, Xiaolan and Wu, Xilun and Gao, Xinbo and Kleinman, Yaniv and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Zhao, Yu and Hao, Yuchen and Qian, Yundi and Li, Yunlu and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei and Ma, Zhiyu},
  date = {2024-11-23},
  eprint = {2407.21783},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2407.21783},
  url = {http://arxiv.org/abs/2407.21783},
  urldate = {2025-07-22},
  abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\LTHY5P65\\Grattafiori 等 - 2024 - The Llama 3 Herd of Models.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\EVHB94PC\\2407.html}
}

@online{gu2025,
  title = {Bridging {{Technology}} and {{Humanities}}: {{Evaluating}} the {{Impact}} of {{Large Language Models}} on {{Social Sciences Research}} with {{DeepSeek-R1}}},
  shorttitle = {Bridging {{Technology}} and {{Humanities}}},
  author = {Gu, Peiran and Duan, Fuhao and Li, Wenhao and Xu, Bochen and Cai, Ying and Yao, Teng and Zhuo, Chenxun and Liu, Tianming and Ge, Bao},
  date = {2025-04-15},
  eprint = {2503.16304},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.16304},
  url = {http://arxiv.org/abs/2503.16304},
  urldate = {2025-07-20},
  abstract = {In recent years, the development of Large Language Models (LLMs) has made significant breakthroughs in the field of natural language processing and has gradually been applied to the field of humanities and social sciences research. LLMs have a wide range of application value in the field of humanities and social sciences because of its strong text understanding, generation and reasoning capabilities. In humanities and social sciences research, LLMs can analyze large-scale text data and make inferences. This article analyzes the large language model DeepSeek-R1 from seven aspects: low-resource language translation, educational question-answering, student writing improvement in higher education, logical reasoning, educational measurement and psychometrics, public health policy analysis, and art education . Then we compare the answers given by DeepSeek-R1 in the seven aspects with the answers given by o1-preview. DeepSeek-R1 performs well in the humanities and social sciences, answering most questions correctly and logically, and can give reasonable analysis processes and explanations. Compared with o1-preview, it can automatically generate reasoning processes and provide more detailed explanations, which is suitable for beginners or people who need to have a detailed understanding of this knowledge, while o1-preview is more suitable for quick reading. Through analysis, it is found that LLM has broad application potential in the field of humanities and social sciences, and shows great advantages in improving text analysis efficiency, language communication and other fields. LLM's powerful language understanding and generation capabilities enable it to deeply explore complex problems in the field of humanities and social sciences, and provide innovative tools for academic research and practical applications.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society},
  file = {C:\Users\bdh19\Zotero\storage\RSCWHWCF\Gu 等 - 2025 - Bridging Technology and Humanities Evaluating the.pdf}
}

@online{liu2024,
  title = {From {{ChatGPT}}, {{DALL-E}} 3 to {{Sora}}: {{How}} has {{Generative AI Changed Digital Humanities Research}} and {{Services}}?},
  shorttitle = {From {{ChatGPT}}, {{DALL-E}} 3 to {{Sora}}},
  author = {Liu, Jiangfeng and Wang, Ziyi and Xie, Jing and Pei, Lei},
  date = {2024-04-29},
  eprint = {2404.18518},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.18518},
  url = {http://arxiv.org/abs/2404.18518},
  urldate = {2025-07-20},
  abstract = {Generative large-scale language models create the fifth paradigm of scientific research, organically combine data science and computational intelligence, transform the research paradigm of natural language processing and multimodal information processing, promote the new trend of AI-enabled social science research, and provide new ideas for digital humanities research and application. This article profoundly explores the application of large-scale language models in digital humanities research, revealing their significant potential in ancient book protection, intelligent processing, and academic innovation. The article first outlines the importance of ancient book resources and the necessity of digital preservation, followed by a detailed introduction to developing large-scale language models, such as ChatGPT, and their applications in document management, content understanding, and cross-cultural research. Through specific cases, the article demonstrates how AI can assist in the organization, classification, and content generation of ancient books. Then, it explores the prospects of AI applications in artistic innovation and cultural heritage preservation. Finally, the article explores the challenges and opportunities in the interaction of technology, information, and society in the digital humanities triggered by AI technologies.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Digital Libraries},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\5SP4ERIK\\Liu 等 - 2024 - From ChatGPT, DALL-E 3 to Sora How has Generative.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\TUUU5I5I\\2404.html}
}

@online{micikevicius2018,
  title = {Mixed {{Precision Training}}},
  author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
  date = {2018-02-15},
  eprint = {1710.03740},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1710.03740},
  url = {http://arxiv.org/abs/1710.03740},
  urldate = {2025-07-22},
  abstract = {Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\H66Y2RHM\\Micikevicius 等 - 2018 - Mixed Precision Training.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\7DNY8WAZ\\1710.html}
}

@article{qu2024,
  title = {Performance and biases of {{Large Language Models}} in public opinion simulation},
  author = {Qu, Yao and Wang, Jue},
  date = {2024-08-28},
  journaltitle = {Humanities and Social Sciences Communications},
  shortjournal = {Humanit Soc Sci Commun},
  volume = {11},
  number = {1},
  pages = {1095},
  publisher = {Palgrave},
  issn = {2662-9992},
  doi = {10.1057/s41599-024-03609-x},
  url = {https://www.nature.com/articles/s41599-024-03609-x},
  urldate = {2025-07-20},
  abstract = {The rise of Large Language Models (LLMs) like ChatGPT marks a pivotal advancement in artificial intelligence, reshaping the landscape of data analysis and processing. By simulating public opinion, ChatGPT shows promise in facilitating public policy development. However, challenges persist regarding its worldwide applicability and bias across demographics and themes. Our research employs socio-demographic data from the World Values Survey to evaluate ChatGPT’s performance in diverse contexts. Findings indicate significant performance disparities, especially when comparing countries. Models perform better in Western, English-speaking, and developed nations, notably the United States, in comparison to others. Disparities also manifest across demographic groups, showing biases related to gender, ethnicity, age, education, and social class. The study further uncovers thematic biases in political and environmental simulations. These results highlight the need to enhance LLMs’ representativeness and address biases, ensuring their equitable and effective integration into public opinion research alongside conventional methodologies.},
  langid = {english},
  keywords = {Politics and international relations,Science,technology and society},
  file = {C:\Users\bdh19\Zotero\storage\8WM5MNGL\Qu 和 Wang - 2024 - Performance and biases of Large Language Models in.pdf}
}

@article{ziems2024,
  title = {Can {{Large Language Models Transform Computational Social Science}}?},
  author = {Ziems, Caleb and Held, William and Shaikh, Omar and Chen, Jiaao and Zhang, Zhehao and Yang, Diyi},
  date = {2024-03-01},
  journaltitle = {Computational Linguistics},
  shortjournal = {Computational Linguistics},
  volume = {50},
  number = {1},
  pages = {237--291},
  issn = {0891-2017},
  doi = {10.1162/coli_a_00502},
  url = {https://doi.org/10.1162/coli_a_00502},
  urldate = {2025-07-20},
  abstract = {Large language models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the computational social science (CSS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers’ gold references. We conclude that the performance of today’s LLMs can augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in social science analysis in partnership with humans.},
  file = {C:\Users\bdh19\Zotero\storage\M6N5WR3E\Ziems 等 - 2024 - Can Large Language Models Transform Computational .pdf}
}

@online{zotero-650,
  title = {Transformer {{Encoder}} for {{Social Science This}} project is funded by {{Niehaus Center}} for {{Globalization}} and {{Governance}}. {{We}} are grateful for the advice from {{David Turner}}, {{Marc Ratkovic}}, {{Walter Mebane}}, {{Sirus Bouchat}}, {{Brian Klobucher}}, and {{Pradhan Abhijeet}}. {{GitHub}}:{{https://github.com/haosenge/TESS,}} {{Hugging Face}}: {{https://huggingface.co/hsge/TESS\_768\_v1}}},
  shorttitle = {Transformer {{Encoder}} for {{Social Science This}} project is funded by {{Niehaus Center}} for {{Globalization}} and {{Governance}}. {{We}} are grateful for the advice from {{David Turner}}, {{Marc Ratkovic}}, {{Walter Mebane}}, {{Sirus Bouchat}}, {{Brian Klobucher}}, and {{Pradhan Abhijeet}}. {{GitHub}}},
  url = {https://ar5iv.labs.arxiv.org/html/2208.08005},
  urldate = {2025-07-20},
  abstract = {High-quality text data has become an important data source for social scientists. We have witnessed the success of pretrained deep neural network models, such as BERT and RoBERTa, in recent social science research. In …},
  langid = {english},
  organization = {ar5iv}
}

@online{deepseek-ai2025,
  title = {{{DeepSeek-R1}}: {{Incentivizing Reasoning Capability}} in {{LLMs}} via {{Reinforcement Learning}}},
  shorttitle = {{{DeepSeek-R1}}},
  author = {DeepSeek-AI and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
  date = {2025-01-22},
  eprint = {2501.12948},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.12948},
  url = {http://arxiv.org/abs/2501.12948},
  urldate = {2025-07-22},
  abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\9XRW8ICI\\DeepSeek-AI 等 - 2025 - DeepSeek-R1 Incentivizing Reasoning Capability in.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\6VKUY7PF\\2501.html}
}

@online{yenduri2023,
  title = {Generative {{Pre-trained Transformer}}: {{A Comprehensive Review}} on {{Enabling Technologies}}, {{Potential Applications}}, {{Emerging Challenges}}, and {{Future Directions}}},
  shorttitle = {Generative {{Pre-trained Transformer}}},
  author = {Yenduri, Gokul and M, Ramalingam and G, Chemmalar Selvi and Y, Supriya and Srivastava, Gautam and Maddikunta, Praveen Kumar Reddy and G, Deepti Raj and Jhaveri, Rutvij H. and B, Prabadevi and Wang, Weizheng and Vasilakos, Athanasios V. and Gadekallu, Thippa Reddy},
  date = {2023-05-21},
  eprint = {2305.10435},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.10435},
  url = {http://arxiv.org/abs/2305.10435},
  urldate = {2025-07-22},
  abstract = {The Generative Pre-trained Transformer (GPT) represents a notable breakthrough in the domain of natural language processing, which is propelling us toward the development of machines that can understand and communicate using language in a manner that closely resembles that of humans. GPT is based on the transformer architecture, a deep neural network designed for natural language processing tasks. Due to their impressive performance on natural language processing tasks and ability to effectively converse, GPT have gained significant popularity among researchers and industrial communities, making them one of the most widely used and effective models in natural language processing and related fields, which motivated to conduct this review. This review provides a detailed overview of the GPT, including its architecture, working process, training procedures, enabling technologies, and its impact on various applications. In this review, we also explored the potential challenges and limitations of a GPT. Furthermore, we discuss potential solutions and future directions. Overall, this paper aims to provide a comprehensive understanding of GPT, enabling technologies, their impact on various applications, emerging challenges, and potential solutions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\Y8YRW24A\\Yenduri 等 - 2023 - Generative Pre-trained Transformer A Comprehensiv.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\FGFY7T44\\2305.html}
}

@online{zotero-722,
  title = {Journal officiel de la {{République}} française. {{Débats}} parlementaires. {{Chambre}} des députés : compte rendu in-extenso - 61 années disponibles - {{Gallica}}},
  url = {https://gallica.bnf.fr/ark:/12148/cb328020951/date.r=Journal+officiel+de+la+Republique+francaise+Debats+parlementaires,+Chambre+des+deputes.langFR},
  urldate = {2025-07-27},
  file = {C:\Users\bdh19\Zotero\storage\TBS8NYMR\date.r=Journal+officiel+de+la+Republique+francaise+Debats+parlementaires,+Chambre+des+deputes.html}
}

@online{zotero-724,
  title = {Journal officiel de la {{République}} française. {{Débats}} parlementaires. {{Sénat}} (1880) - 60 années disponibles - {{Gallica}}},
  url = {https://gallica.bnf.fr/ark:/12148/cb34363182v/date},
  urldate = {2025-07-27},
  file = {C:\Users\bdh19\Zotero\storage\5FLAIUR9\date.html}
}

@unpublished{spacy2,
    AUTHOR = {Honnibal, Matthew and Montani, Ines},
    TITLE  = {{spaCy 2}: Natural language understanding with {B}loom embeddings, convolutional neural networks and incremental parsing},
    YEAR   = {2017},
    Note   = {To appear}
}

@online{2024d,
  title = {knowledgator/gliner-multitask-v1.0 · {{Hugging Face}}},
  date = {2024-07-15},
  url = {https://huggingface.co/knowledgator/gliner-multitask-v1.0},
  urldate = {2025-07-28},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {C:\Users\bdh19\Zotero\storage\RLTHBZ8Q\gliner-multitask-v1.html}
}

@online{2024e,
  title = {sentence-transformers/all-{{MiniLM-L6-v2}} · {{Hugging Face}}},
  date = {2024-01-05},
  url = {https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2},
  urldate = {2025-07-28},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {C:\Users\bdh19\Zotero\storage\YPWMZLGY\all-MiniLM-L6-v2.html}
}

@inproceedings{bird2004,
  title = {{{NLTK}}: {{The Natural Language Toolkit}}},
  shorttitle = {{{NLTK}}},
  booktitle = {Proceedings of the {{ACL Interactive Poster}} and {{Demonstration Sessions}}},
  author = {Bird, Steven and Loper, Edward},
  date = {2004-07},
  pages = {214--217},
  publisher = {Association for Computational Linguistics},
  location = {Barcelona, Spain},
  url = {https://aclanthology.org/P04-3031/},
  urldate = {2025-07-28},
  file = {C:\Users\bdh19\Zotero\storage\S9FJUJT4\Bird 和 Loper - 2004 - NLTK The Natural Language Toolkit.pdf}
}

@online{levenshteyn,
  title   = {V.~I.~Levenshtein, “Binary codes capable of correcting deletions, insertions, and reversals”},
  author  = {Levenshtein, V. I.},
  journal = {Doklady Akademii Nauk SSSR},
  volume  = {163},
  number  = {4},
  year    = {1965},
  pages   = {845--848},
  url     = {https://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=dan&paperid=31411&option_lang=rus},
  urldate = {2025-07-28}
}

@online{zotero-734,
  title = {rapidfuzz.fuzz - {{RapidFuzz}} 3.13.0 documentation},
  url = {https://rapidfuzz.github.io/RapidFuzz/Usage/fuzz.html},
  urldate = {2025-07-28},
  file = {C:\Users\bdh19\Zotero\storage\ENC5NVZH\fuzz.html}
}



@online{2024f,
  title = {meta-llama/{{Llama-3}}.1-{{8B}} · {{Hugging Face}}},
  date = {2024-12-06},
  url = {https://huggingface.co/meta-llama/Llama-3.1-8B},
  urldate = {2025-08-02},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {C:\Users\bdh19\Zotero\storage\IUZ7A8QP\Llama-3.html}
}

@online{2025,
  title = {unsloth/{{Meta-Llama-3}}.1-{{8B-Instruct-bnb-4bit}} · {{Hugging Face}}},
  date = {2025-07-30},
  url = {https://huggingface.co/unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit},
  urldate = {2025-08-02},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {C:\Users\bdh19\Zotero\storage\BN5W7EMH\Meta-Llama-3.html}
}

@online{2025a,
  title = {Gemini {{CLI}}: your open-source {{AI}} agent},
  shorttitle = {Gemini {{CLI}}},
  date = {2025-06-25},
  url = {https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/},
  urldate = {2025-08-02},
  abstract = {Free and open source, Gemini CLI brings Gemini directly into developers’ terminals — with unmatched access for individuals.},
  langid = {english},
  organization = {Google}
}

@inproceedings{ramos2003,
  title = {Using {{TF-IDF}} to {{Determine Word Relevance}} in {{Document Queries}}},
  author = {Ramos, J. E.},
  date = {2003},
  url = {https://www.semanticscholar.org/paper/Using-TF-IDF-to-Determine-Word-Relevance-in-Queries-Ramos/b3bf6373ff41a115197cb5b30e57830c16130c2c},
  urldate = {2025-08-02},
  abstract = {In this paper, we examine the results of applying Term Frequency Inverse Document Frequency (TF-IDF) to determine what words in a corpus of documents might be more favorable to use in a query. As the term implies, TF-IDF calculates values for each word in a document through an inverse proportion of the frequency of the word in a particular document to the percentage of documents the word appears in. Words with high TF-IDF numbers imply a strong relationship with the document they appear in, suggesting that if that word were to appear in a query, the document could be of interest to the user. We provide evidence that this simple algorithm efficiently categorizes relevant words that can enhance query retrieval.}
}

@online{zotero-743,
  title = {Models {{Overview}} | {{Mistral AI}}},
  url = {https://docs.mistral.ai/getting-started/models/models_overview/},
  urldate = {2025-08-02},
  abstract = {Mistral provides two types of models: open models and premier models.},
  langid = {english}
}

@online{zotero-754,
  title = {llama3.1},
  url = {https://ollama.com/llama3.1},
  urldate = {2025-08-03},
  abstract = {Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.},
  file = {C:\Users\bdh19\Zotero\storage\IMLFYHW2\llama3.html}
}

@online{zotero-756,
  title = {nomic-embed-text},
  url = {https://ollama.com/nomic-embed-text},
  urldate = {2025-08-03},
  abstract = {A high-performing open embedding model with a large token context window.},
  file = {C:\Users\bdh19\Zotero\storage\NKEYN7BA\nomic-embed-text.html}
}

@incollection{yu2025,
  title = {Evaluation of {{Retrieval-Augmented Generation}}: {{A Survey}}},
  shorttitle = {Evaluation of {{Retrieval-Augmented Generation}}},
  author = {Yu, Hao and Gan, Aoran and Zhang, Kai and Tong, Shiwei and Liu, Qi and Liu, Zhaofeng},
  date = {2025},
  volume = {2301},
  eprint = {2405.07437},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {102--120},
  doi = {10.1007/978-981-96-1024-2_8},
  url = {http://arxiv.org/abs/2405.07437},
  urldate = {2025-08-03},
  abstract = {Retrieval-Augmented Generation (RAG) has recently gained traction in natural language processing. Numerous studies and real-world applications are leveraging its ability to enhance generative models through external information retrieval. Evaluating these RAG systems, however, poses unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. To better understand these challenges, we conduct A Unified Evaluation Process of RAG (Auepora) and aim to provide a comprehensive overview of the evaluation and benchmarks of RAG systems. Specifically, we examine and compare several quantifiable metrics of the Retrieval and Generation components, such as relevance, accuracy, and faithfulness, within the current RAG benchmarks, encompassing the possible output and ground truth pairs. We then analyze the various datasets and metrics, discuss the limitations of current benchmarks, and suggest potential directions to advance the field of RAG benchmarks.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\D2GXFL9P\\Yu 等 - 2025 - Evaluation of Retrieval-Augmented Generation A Su.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\8FGW3NSB\\2405.html}
}

@online{zotero-749,
  title = {{{RAG Evaluation}} - {{Hugging Face Open-Source AI Cookbook}}},
  url = {https://huggingface.co/learn/cookbook/rag_evaluation},
  urldate = {2025-08-03},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {C:\Users\bdh19\Zotero\storage\U6VE3SDE\rag_evaluation.html}
}

@online{2025b,
  title = {{{HenriPorteur}}/ocr-error-senat · {{Datasets}} at {{Hugging Face}}},
  date = {2025-02-24},
  url = {https://huggingface.co/datasets/HenriPorteur/ocr-error-senat},
  urldate = {2025-08-03},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.}
}

@inproceedings{thomas2024,
  title = {Leveraging {{LLMs}} for {{Post-OCR Correction}} of {{Historical Newspapers}}},
  booktitle = {Proceedings of the {{Third Workshop}} on {{Language Technologies}} for {{Historical}} and {{Ancient Languages}} ({{LT4HALA}}) @ {{LREC-COLING-2024}}},
  author = {Thomas, Alan and Gaizauskas, Robert and Lu, Haiping},
  editor = {Sprugnoli, Rachele and Passarotti, Marco},
  date = {2024-05},
  pages = {116--121},
  publisher = {{ELRA and ICCL}},
  location = {Torino, Italia},
  url = {https://aclanthology.org/2024.lt4hala-1.14/},
  urldate = {2025-08-03},
  abstract = {Poor OCR quality continues to be a major obstacle for humanities scholars seeking to make use of digitised primary sources such as historical newspapers. Typical approaches to post-OCR correction employ sequence-to-sequence models for a neural machine translation task, mapping erroneous OCR texts to accurate reference texts. We shift our focus towards the adaptation of generative LLMs for a prompt-based approach. By instruction-tuning Llama 2 and comparing it to a fine-tuned BART on BLN600, a parallel corpus of 19th century British newspaper articles, we demonstrate the potential of a prompt-based approach in detecting and correcting OCR errors, even with limited training data. We achieve a significant enhancement in OCR quality with Llama 2 outperforming BART, achieving a 54.51\% reduction in the character error rate against BART's 23.30\%. This paves the way for future work leveraging generative LLMs to improve the accessibility and unlock the full potential of historical texts for humanities research.},
  file = {C:\Users\bdh19\Zotero\storage\QBIH84FF\Thomas 等 - 2024 - Leveraging LLMs for Post-OCR Correction of Histori.pdf}
}

@online{zotero-758,
  title = {La base {{Comptes}} rendus : data.senat.fr},
  url = {https://data.senat.fr/la-base-comptes-rendus/},
  urldate = {2025-08-03},
  file = {C:\Users\bdh19\Zotero\storage\3RSL4G56\la-base-comptes-rendus.html}
}

@online{2025c,
  title = {{{HenriPorteur}}/bart-large-ocr-fr · {{Hugging Face}}},
  date = {2025-02-23},
  url = {https://huggingface.co/HenriPorteur/bart-large-ocr-fr},
  urldate = {2025-08-03},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {C:\Users\bdh19\Zotero\storage\WR86D5EU\bart-large-ocr-fr.html}
}

@article{hu2006,
  title = {Efficient, {{High-Quality Force-Directed Graph Drawing}}},
  author = {Hu, Yifan},
  date = {2006},
  journaltitle = {The Mathematica journal},
  url = {https://www.semanticscholar.org/paper/Efficient%2C-High-Quality-Force-Directed-Graph-Hu/be33ebd01f336c04a1db20830576612ab45b1b9b},
  urldate = {2025-08-03},
  abstract = {We propose a graph drawing algorithm that is both efficient and high quality. This algorithm combines a multilevel approach, which effectively overcomes local minimums, with the Barnes and Hut [1] octree technique, which approximates shortand long-range force efficiently. Our numerical results show that the algorithm is comparable in speed to Walshaw’s [2] highly efficient multilevel graph drawing algorithm, yet gives better results on some of the difficult problems. In addition, an adaptive cooling scheme for the force-directed algorithms and a general repulsive force model are proposed. The proposed graph drawing algorithm and others are included with Mathematica 5.1 and later versions in the package DiscreteMath‘GraphÑ Plot.}
}

@online{zotero-768,
  title = {Hallucination {{Evaluation Leaderboard}} - a {{Hugging Face Space}} by vectara},
  url = {https://huggingface.co/spaces/vectara/Hallucination-evaluation-leaderboard},
  urldate = {2025-08-03},
  abstract = {Discover amazing ML apps made by the community},
  file = {C:\Users\bdh19\Zotero\storage\WW6ME98W\Hallucination-evaluation-leaderboard.html}
}

@article{blondel2008,
  title = {Fast unfolding of communities in large networks},
  author = {Blondel, Vincent D. and Guillaume, Jean-Loup and Lambiotte, Renaud and Lefebvre, Etienne},
  date = {2008-10-01},
  journaltitle = {Journal of Statistical Mechanics: Theory and Experiment},
  shortjournal = {J. Stat. Mech.},
  volume = {2008},
  number = {10},
  eprint = {0803.0476},
  eprinttype = {arXiv},
  eprintclass = {physics},
  pages = {P10008},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/2008/10/P10008},
  url = {http://arxiv.org/abs/0803.0476},
  urldate = {2025-08-04},
  abstract = {We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection method in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2.6 million customers and by analyzing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad-hoc modular networks. .},
  keywords = {Computer Science - Computers and Society,Computer Science - Data Structures and Algorithms,Condensed Matter - Statistical Mechanics,Physics - Physics and Society},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\GJTB46VJ\\Blondel 等 - 2008 - Fast unfolding of communities in large networks.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\B65Z2VKE\\0803.html}
}

@inproceedings{martin2011,
  title = {{{OpenOrd}}: an open-source toolbox for large graph layout},
  shorttitle = {{{OpenOrd}}},
  booktitle = {Visualization and {{Data Analysis}} 2011},
  author = {Martin, Shawn and Brown, W. Michael and Klavans, Richard and Boyack, Kevin W.},
  date = {2011-01-24},
  volume = {7868},
  pages = {45--55},
  publisher = {SPIE},
  doi = {10.1117/12.871402},
  url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/7868/786806/OpenOrd-an-open-source-toolbox-for-large-graph-layout/10.1117/12.871402.full},
  urldate = {2025-08-04},
  abstract = {We document an open-source toolbox for drawing large-scale undirected graphs. This toolbox is based on a previously implemented closed-source algorithm known as VxOrd. Our toolbox, which we call OpenOrd, extends the capabilities of VxOrd to large graph layout by incorporating edge-cutting, a multi-level approach, average-link clustering, and a parallel implementation. At each level, vertices are grouped using force-directed layout and average-link clustering. The clustered vertices are then re-drawn and the process is repeated. When a suitable drawing of the coarsened graph is obtained, the algorithm is reversed to obtain a drawing of the original graph. This approach results in layouts of large graphs which incorporate both local and global structure. A detailed description of the algorithm is provided in this paper. Examples using datasets with over 600K nodes are given. Code is available at www.cs.sandia.gov/\textasciitilde smartin.},
  eventtitle = {Visualization and {{Data Analysis}} 2011}
}

@article{traag2019,
  title = {From {{Louvain}} to {{Leiden}}: guaranteeing well-connected communities},
  shorttitle = {From {{Louvain}} to {{Leiden}}},
  author = {Traag, Vincent and Waltman, Ludo and family=Eck, given=Nees Jan, prefix=van, useprefix=false},
  date = {2019-03-26},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {9},
  number = {1},
  eprint = {1810.08473},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {5233},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-41695-z},
  url = {http://arxiv.org/abs/1810.08473},
  urldate = {2025-08-04},
  abstract = {Community detection is often used to understand the structure of large and complex networks. One of the most popular algorithms for uncovering community structure is the so-called Louvain algorithm. We show that this algorithm has a major defect that largely went unnoticed until now: the Louvain algorithm may yield arbitrarily badly connected communities. In the worst case, communities may even be disconnected, especially when running the algorithm iteratively. In our experimental analysis, we observe that up to 25\% of the communities are badly connected and up to 16\% are disconnected. To address this problem, we introduce the Leiden algorithm. We prove that the Leiden algorithm yields communities that are guaranteed to be connected. In addition, we prove that, when the Leiden algorithm is applied iteratively, it converges to a partition in which all subsets of all communities are locally optimally assigned. Furthermore, by relying on a fast local move approach, the Leiden algorithm runs faster than the Louvain algorithm. We demonstrate the performance of the Leiden algorithm for several benchmark and real-world networks. We find that the Leiden algorithm is faster than the Louvain algorithm and uncovers better partitions, in addition to providing explicit guarantees.},
  keywords = {Computer Science - Social and Information Networks,Physics - Physics and Society},
  file = {C:\Users\bdh19\Zotero\storage\2YEAABCE\Traag 等 - 2019 - From Louvain to Leiden guaranteeing well-connecte.pdf}
}

@online{zotero-777,
  title = {Document - {{GraphRAG}}},
  url = {https://microsoft.github.io/graphrag/data/operation_dulce/Operation%20Dulce%20v2%201%201/},
  urldate = {2025-08-04},
  file = {C:\Users\bdh19\Zotero\storage\N6QMDUZZ\Operation Dulce v2 1 1.html}
}

@online{zotero-779,
  title = {{{LightRAG}}/lightrag/prompt.py at main · {{HKUDS}}/{{LightRAG}}},
  url = {https://github.com/HKUDS/LightRAG/blob/main/lightrag/prompt.py},
  urldate = {2025-08-06},
  file = {C:\Users\bdh19\Zotero\storage\FIUHB9Y6\prompt.html}
}


@online{hsieh2024,
  title = {{{RULER}}: {{What}}'s the {{Real Context Size}} of {{Your Long-Context Language Models}}?},
  shorttitle = {{{RULER}}},
  author = {Hsieh, Cheng-Ping and Sun, Simeng and Kriman, Samuel and Acharya, Shantanu and Rekesh, Dima and Jia, Fei and Zhang, Yang and Ginsburg, Boris},
  date = {2024-08-06},
  eprint = {2404.06654},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.06654},
  url = {http://arxiv.org/abs/2404.06654},
  urldate = {2025-08-17},
  abstract = {The needle-in-a-haystack (NIAH) test, which examines the ability to retrieve a piece of information (the "needle") from long distractor texts (the "haystack"), has been widely adopted to evaluate long-context language models (LMs). However, this simple retrieval-based test is indicative of only a superficial form of long-context understanding. To provide a more comprehensive evaluation of long-context LMs, we create a new synthetic benchmark RULER with flexible configurations for customized sequence length and task complexity. RULER expands upon the vanilla NIAH test to encompass variations with diverse types and quantities of needles. Moreover, RULER introduces new task categories multi-hop tracing and aggregation to test behaviors beyond searching from context. We evaluate 17 long-context LMs with 13 representative tasks in RULER. Despite achieving nearly perfect accuracy in the vanilla NIAH test, almost all models exhibit large performance drops as the context length increases. While these models all claim context sizes of 32K tokens or greater, only half of them can maintain satisfactory performance at the length of 32K. Our analysis of Yi-34B, which supports context length of 200K, reveals large room for improvement as we increase input length and task complexity. We open source RULER to spur comprehensive evaluation of long-context LMs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\PITVRNRD\\Hsieh 等 - 2024 - RULER What's the Real Context Size of Your Long-C.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\4PAVVV3A\\2404.html}
}

@article{kahler1992,
  title = {Multilateralism with small and large numbers},
  author = {Kahler, Miles},
  date = {1992-07},
  journaltitle = {International Organization},
  volume = {46},
  number = {3},
  pages = {681--708},
  issn = {1531-5088, 0020-8183},
  doi = {10.1017/S0020818300027867},
  url = {https://www.cambridge.org/core/journals/international-organization/article/multilateralism-with-small-and-large-numbers/F92DF0397D019061F9E8759C326CBEEB},
  urldate = {2025-08-15},
  abstract = {Multilateralism, international governance of the “many,” was defined by the United States after 1945 in terms of certain principles, particularly opposition to bilateral and discriminatory arrangements that were believed to enhance the leverage of the powerful over the weak and to increase international conflict. Postwar multilateralism also expressed an impulse to universality (John Ruggie's “generalized organizing principles”) that implied relatively low barriers to participation in these arrangements. A ticket of admission was always required, whether acceding to the General Agreement on Tariffs and Trade (GATT) or joining the International Monetary Fund (IMF) and the World Bank. Nevertheless, the price of that ticket was not set so high that less powerful or less wealthy states could not hope to participate.},
  langid = {english},
  file = {C:\Users\bdh19\Zotero\storage\8H9DFZR9\Kahler - 1992 - Multilateralism with small and large numbers.pdf}
}

@online{liu2023,
  title = {Lost in the {{Middle}}: {{How Language Models Use Long Contexts}}},
  shorttitle = {Lost in the {{Middle}}},
  author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  date = {2023-11-20},
  eprint = {2307.03172},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.03172},
  url = {http://arxiv.org/abs/2307.03172},
  urldate = {2025-08-17},
  abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\bdh19\\Zotero\\storage\\JVBU9FVQ\\Liu 等 - 2023 - Lost in the Middle How Language Models Use Long C.pdf;C\:\\Users\\bdh19\\Zotero\\storage\\6QUUK25K\\2307.html}
}

@online{nationsb,
  title = {International {{Day}} of {{Multilateralism}} and {{Diplomacy}} for {{Peace}}},
  author = {Nations, United},
  publisher = {United Nations},
  url = {https://www.un.org/en/observances/multilateralism-for-peace-day},
  urldate = {2025-08-15},
  abstract = {International Day of Multilateralism and Diplomacy for Peace is a reaffirmation of the UN charter and its principles of resolving disputes among countries through peaceful means. It acknowledges the use of multilateral decision-making and diplomacy in achieving peaceful resolutions to conflicts among nations.},
  langid = {english},
  organization = {United Nations},
  file = {C:\Users\bdh19\Zotero\storage\SR9XYWHB\multilateralism-for-peace-day.html}
}

@online{2025d,
  title = {Hugging {{Face}} – {{The AI}} community building the future.},
  date = {2025-08-17},
  url = {https://huggingface.co/},
  urldate = {2025-08-17},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.}
}

@online{zotero-794,
  title = {{{vLLM}}},
  url = {https://docs.vllm.ai/en/latest/},
  urldate = {2025-08-17}
}

@online{zotero-795,
  title = {Ollama},
  url = {https://ollama.com},
  urldate = {2025-08-17},
  abstract = {Get up and running with large language models.},
  file = {C:\Users\bdh19\Zotero\storage\HL8WSEET\ollama.com.html}
}
