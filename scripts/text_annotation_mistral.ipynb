{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mistralai\n",
      "  Downloading mistralai-1.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting eval-type-backport>=0.2.0 (from mistralai)\n",
      "  Using cached eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /home/donghan/anaconda3/envs/graphrag/lib/python3.13/site-packages (from mistralai) (0.28.1)\n",
      "Collecting jsonpath-python>=1.0.6 (from mistralai)\n",
      "  Using cached jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: pydantic>=2.9.0 in /home/donghan/anaconda3/envs/graphrag/lib/python3.13/site-packages (from mistralai) (2.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/donghan/anaconda3/envs/graphrag/lib/python3.13/site-packages (from mistralai) (2.9.0.post0)\n",
      "Collecting typing-inspect>=0.9.0 (from mistralai)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: anyio in /home/donghan/anaconda3/envs/graphrag/lib/python3.13/site-packages (from httpx>=0.27.0->mistralai) (4.8.0)\n",
      "Requirement already satisfied: certifi in /home/donghan/anaconda3/envs/graphrag/lib/python3.13/site-packages (from httpx>=0.27.0->mistralai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/donghan/anaconda3/envs/graphrag/lib/python3.13/site-packages (from httpx>=0.27.0->mistralai) (1.0.7)\n",
      "Requirement already satisfied: idna in /home/donghan/anaconda3/envs/graphrag/lib/python3.13/site-packages (from httpx>=0.27.0->mistralai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/donghan/anaconda3/envs/graphrag/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.27.0->mistralai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/donghan/anaconda3/envs/graphrag/lib/python3.13/site-packages (from pydantic>=2.9.0->mistralai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/donghan/anaconda3/envs/graphrag/lib/python3.13/site-packages (from pydantic>=2.9.0->mistralai) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/donghan/anaconda3/envs/graphrag/lib/python3.13/site-packages (from pydantic>=2.9.0->mistralai) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/donghan/anaconda3/envs/graphrag/lib/python3.13/site-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.9.0->mistralai)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/donghan/anaconda3/envs/graphrag/lib/python3.13/site-packages (from anyio->httpx>=0.27.0->mistralai) (1.3.1)\n",
      "Downloading mistralai-1.5.0-py3-none-any.whl (271 kB)\n",
      "Using cached eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Using cached jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, jsonpath-python, eval-type-backport, typing-inspect, mistralai\n",
      "Successfully installed eval-type-backport-0.2.2 jsonpath-python-1.0.6 mistralai-1.5.0 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "! pip install mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from mistralai import Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chambre_int = pd.read_csv('df_chambre_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = ''\n",
    "model = \"mistral-small-latest\"\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "def classify_text(text):\n",
    "\n",
    "    prompt = (\n",
    "        \"Veuillez déterminer si le texte suivant traite des relations entre la France et d'autres pays, \"\n",
    "        \"ou d'événements spécifiques de la politique internationale en dehors de la France. Veuillez juger avec rigueur. \"\n",
    "        \"Si c'est le cas, retournez 1 ; sinon, ou si le texte ne fait que mentionner des noms liés, retournez 0.\\n\"\n",
    "        \"Vous devez strictement respecter le format requis : vous ne pouvez répondre que par 0 ou 1, rien d'autre.\\n\"\n",
    "        f\"Texte : {text}\\n\"\n",
    "        \"Votre réponse (uniquement 0 ou 1) :\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    max_retries = 3\n",
    "    retry_delay = 2  \n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.complete(\n",
    "                model=\"mistral-small-latest\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}],\n",
    "                stream=False,\n",
    "            )\n",
    "            \n",
    "            llm_reply = response.choices[0].message.content.strip()\n",
    "            \n",
    "\n",
    "            print(f\"Text: {text[:50]}...\\nLLM Reply: {llm_reply}\\n\")\n",
    "            time.sleep(0.5)\n",
    "\n",
    "            if llm_reply in [\"0\", \"1\"]:\n",
    "                return int(llm_reply)\n",
    "            else:\n",
    "                print(f\"Error in LLM's response: {llm_reply}, trying to extract numbers\")\n",
    "            \n",
    "                for char in llm_reply:\n",
    "                    if char in [\"0\", \"1\"]:\n",
    "                        return int(char)\n",
    "                        \n",
    "                print(f\"Can't extract number, use -1 to represent anormaly\")\n",
    "                return -1  \n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Erro (Retry in {attempt+1}/{max_retries}): {e}，{retry_delay}\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(f\"Error, max time reached: {e}\")\n",
    "                return -1  \n",
    "\n",
    "def process_dataframe_with_checkpoint(df, text_column, result_column, checkpoint_file=\"classification_checkpoint.csv\"):\n",
    "    \"\"\"Save and load checkpoint\"\"\"\n",
    "    \n",
    "\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        print(f\"Find checkppoint: {checkpoint_file}, Resuming\")\n",
    "        df_checkpoint = pd.read_csv(checkpoint_file)\n",
    "        \n",
    "\n",
    "        if 'index' in df_checkpoint.columns:\n",
    "            processed_indices = df_checkpoint['index'].tolist()\n",
    "            for idx in processed_indices:\n",
    "                if idx < len(df):\n",
    "                    df.at[idx, result_column] = df_checkpoint.loc[df_checkpoint['index'] == idx, result_column].values[0]\n",
    "            \n",
    "\n",
    "            total_indices = set(range(len(df)))\n",
    "            unprocessed_indices = list(total_indices - set(processed_indices))\n",
    "            unprocessed_indices.sort() \n",
    "        else:\n",
    "\n",
    "            print(\"No index column, start over\")\n",
    "            unprocessed_indices = list(range(len(df)))\n",
    "    else:\n",
    "\n",
    "        print(\"No checkpoint, start over\")\n",
    "        df[result_column] = -2 \n",
    "        unprocessed_indices = list(range(len(df)))\n",
    "\n",
    "    for idx in tqdm(unprocessed_indices, desc=\"Processing\"):\n",
    "        text = df.at[idx, text_column]\n",
    "        result = classify_text(text)\n",
    "        df.at[idx, result_column] = result\n",
    "        \n",
    "\n",
    "        df_checkpoint = pd.DataFrame({\n",
    "            'index': [idx],\n",
    "            text_column: [text],\n",
    "            result_column: [result]\n",
    "        })\n",
    "        \n",
    "\n",
    "        mode = 'a' if os.path.exists(checkpoint_file) and idx != unprocessed_indices[0] else 'w'\n",
    "        header = not (os.path.exists(checkpoint_file) and idx != unprocessed_indices[0])\n",
    "        df_checkpoint.to_csv(checkpoint_file, mode=mode, header=header, index=False)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "df_chambre_int = process_dataframe_with_checkpoint(\n",
    "    df_chambre_int, \n",
    "    text_column=\"Texte\", \n",
    "    result_column=\"is_international_politics\",\n",
    "    checkpoint_file=\"chambre_int_checkpoint.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "df_chambre_int.to_csv(\"chambre_int_processed.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\Abstract:\")\n",
    "print(df_chambre_int[\"is_international_politics\"].value_counts())\n",
    "print(\"\\nFive first rows:\")\n",
    "print(df_chambre_int.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
